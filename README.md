# Awesom-AGI Survey Papers

Must-read Papers on Artifical General Intelligence with foundation models.

---

# üìúContent

- [Awesom-AGI Survey Papers](#awesom-agi-survey-papers)
- [üìúContent](#content)
  - [1. Introduction](#1-introduction)
  - [2. AGI Internal: Unveiling the Mind of AGI](#2-agi-internal-unveiling-the-mind-of-agi)
    - [2.1 Perception](#21-perception)
    - [2.2 Reasoning](#22-reasoning)
    - [2.3 Memory](#23-memory)
    - [2.4 Metacognition](#24-metacognition)
  - [3. AGI Interface: Connecting the World with AGI](#3-agi-interface-connecting-the-world-with-agi)
    - [3.1 Interfaces to Digital World](#31-interfaces-to-digital-world)
    - [3.2 Interfaces to Physical World](#32-interfaces-to-physical-world)
    - [3.3 Interfaces to Intelligence](#33-interfaces-to-intelligence)
      - [3.3.2 Interfaces to Humans](#332-interfaces-to-humans)
  - [4. AGI Systems: Implementing the Mechanism of AGI](#4-agi-systems-implementing-the-mechanism-of-agi)
    - [4.1 System Challenges](#41-system-challenges)
    - [4.2 Scalable Model Architectures](#42-scalable-model-architectures)
    - [4.3 Large-scale Training](#43-large-scale-training)
    - [4.4 Inference Techniques](#44-inference-techniques)
    - [4.5 Cost and Efficiency](#45-cost-and-efficiency)
    - [4.6 Computing Platforms](#46-computing-platforms)
    - [4.7 The Future of AGI Systems](#47-the-future-of-agi-systems)
  - [5. AGI Alignment: Reconciling Needs with AGI](#5-agi-alignment-reconciling-needs-with-agi)
    - [5.1 Expectations of AGI Alignment](#51-expectations-of-agi-alignment)
    - [5.2 AGI Alignment Classifications](#52-agi-alignment-classifications)
    - [5.3 How to Implement: Solutions for Alignment](#53-how-to-implement-solutions-for-alignment)
  - [6. Approach AGI Responsibly](#6-approach-agi-responsibly)
    - [6.1 AI Levels: Charting the Evolution of Artificial Intelligence](#61-ai-levels-charting-the-evolution-of-artificial-intelligence)
      - [6.1.1 AGI Levels](#611-agi-levels)
      - [6.1.2 Constraints and Challenges of Ultimate AGI](#612-constraints-and-challenges-of-ultimate-agi)
      - [6.1.3 How Do We Get to the Next Level of AGI?](#613-how-do-we-get-to-the-next-level-of-agi)
    - [6.2 AGI Evaluation](#62-agi-evaluation)
      - [6.2.1 What Do We Expect from AGI Evaluations](#621-what-do-we-expect-from-agi-evaluations)
      - [6.2.2 Current Evaluation Frameworks and Limitations](#622-current-evaluation-frameworks-and-limitations)
    - [6.3 Potential Ways to Future AGI](#63-potential-ways-to-future-agi)
  - [7. Case Studies](#7-case-studies)
    - [7.1 AI for Science Discovery and Research](#71-ai-for-science-discovery-and-research)
    - [7.2 Generative Visual Intelligence](#72-generative-visual-intelligence)
    - [7.3 World Models](#73-world-models)
    - [7.4 Decentralized LLM](#74-decentralized-llm)
    - [7.5 AI for Coding](#75-ai-for-coding)
    - [7.6 AI for Robotics in Real World Applications](#76-ai-for-robotics-in-real-world-applications)
    - [7.7 Human-AI Collaboration](#77-human-ai-collaboration)
  - [8. Conclusion](#8-conclusion)


## 1. Introduction

## 2. AGI Internal: Unveiling the Mind of AGI
### 2.1 Perception

1. **Experience grounds language** *Bisk, Yonatan, Holtzman, Ari, Thomason, Jesse, Andreas, Jacob, Bengio, Yoshua, Chai, Joyce, Lapata, Mirella, Lazaridou, Angeliki, May, Jonathan, Nisnevich, Aleksandr, others.* arXiv preprint arXiv:2004.10151, 2020. [[abs](https://arxiv.org/abs/2004.10151)]
2. **High-resolution image synthesis with latent diffusion models** *Rombach, Robin, Blattmann, Andreas, Lorenz, Dominik, Esser, Patrick, Ommer, Bj\"o.* Presented at Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022. [Link](No URL)
3. **Flamingo: a visual language model for few-shot learning** *Alayrac, Jean-Baptiste, Donahue, Jeff, Luc, Pauline, Miech, Antoine, Barr, Iain, Hasson, Yana, Lenc, Karel, Mensch, Arthur, Millican, Katherine, Reynolds, Malcolm, others.* No journal, 2022.
4. **Sequential Modeling Enables Scalable Learning for Large Vision Models** *Bai, Yutong, Geng, Xinyang, Mangalam, Karttikeya, Bar, Amir, Yuille, Alan, Darrell, Trevor, Malik, Jitendra, Efros, Alexei A.* arXiv preprint arXiv:2312.00785, 2023. [[abs](https://arxiv.org/abs/2312.00785)]
5. **Voyager: An open-ended embodied agent with large language models** *Wang, Guanzhi, Xie, Yuqi, Jiang, Yunfan, Mandlekar, Ajay, Xiao, Chaowei, Zhu, Yuke, Fan, Linxi, Anandkumar, Anima.* arXiv preprint arXiv:2305.16291, 2023. [[abs](https://arxiv.org/abs/2305.16291)]
6. **Generating images with multimodal language models** *Koh, Jing Yu, Fried, Daniel, Salakhutdinov, Ruslan.* arXiv preprint arXiv:2305.17216, 2023. [[abs](https://arxiv.org/abs/2305.17216)]
7. **What if the tv was off? examining counterfactual reasoning abilities of multi-modal language models** *Zhang, Letian, Zhai, Xiaotong, Zhao, Zhongkai, Wen, Xin, Zhao, Bingchen.* Presented at Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023. [Link](No URL)
8. **Otter: A multi-modal model with in-context instruction tuning** *Li, Bo, Zhang, Yuanhan, Chen, Liangyu, Wang, Jinghao, Yang, Jingkang, Liu, Ziwei.* arXiv preprint arXiv:2305.03726, 2023. [[abs](https://arxiv.org/abs/2305.03726)]
9. **Videochat: Chat-centric video understanding** *Li, KunChang, He, Yinan, Wang, Yi, Li, Yizhuo, Wang, Wenhai, Luo, Ping, Wang, Yali, Wang, Limin, Qiao, Yu.* arXiv preprint arXiv:2305.06355, 2023. [[abs](https://arxiv.org/abs/2305.06355)]
10. **Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents** *Chen, Weize, Su, Yusheng, Zuo, Jingwei, Yang, Cheng, Yuan, Chenfei, Qian, Chen, Chan, Chi-Min, Qin, Yujia, Lu, Yaxi, Xie, Ruobing, others.* arXiv preprint arXiv:2308.10848, 2023. [[abs](https://arxiv.org/abs/2308.10848)]
11. **X-llm: Bootstrapping advanced large language models by treating multi-modalities as foreign languages** *Chen, Feilong, Han, Minglun, Zhao, Haozhi, Zhang, Qingyang, Shi, Jing, Xu, Shuang, Xu, Bo.* arXiv preprint arXiv:2305.04160, 2023. [[abs](https://arxiv.org/abs/2305.04160)]
12. **Cheap and quick: Efficient vision-language instruction tuning for large language models** *Luo, Gen, Zhou, Yiyi, Ren, Tianhe, Chen, Shengxin, Sun, Xiaoshuai, Ji, Rongrong.* arXiv preprint arXiv:2305.15023, 2023. [[abs](https://arxiv.org/abs/2305.15023)]
13. **LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents** *Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun Zhu, Lei Zhang, Jianfeng Gao, Chunyuan Li.* arXiv preprint arXiv:2311.05437, 2023. [[abs](https://arxiv.org/abs/2311.05437)]
14. **Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration** *Lyu, Chenyang, Wu, Minghao, Wang, Longyue, Huang, Xinting, Liu, Bingshuai, Du, Zefeng, Shi, Shuming, Tu, Zhaopeng.* arXiv preprint arXiv:2306.09093, 2023. [[abs](https://arxiv.org/abs/2306.09093)]
15. **mplug-owl: Modularization empowers large language models with multimodality** *Ye, Qinghao, Xu, Haiyang, Xu, Guohai, Ye, Jiabo, Yan, Ming, Zhou, Yiyang, Wang, Junyang, Hu, Anwen, Shi, Pengcheng, Shi, Yaya, others.* arXiv preprint arXiv:2304.14178, 2023. [[abs](https://arxiv.org/abs/2304.14178)]
16. **A Survey on Multimodal Large Language Models** *Yin, Shukang, Fu, Chaoyou, Zhao, Sirui, Li, Ke, Sun, Xing, Xu, Tong, Chen, Enhong.* arXiv preprint arXiv:2306.13549, 2023. [[abs](https://arxiv.org/abs/2306.13549)]
17. **Imagebind-llm: Multi-modality instruction tuning** *Han, Jiaming, Zhang, Renrui, Shao, Wenqi, Gao, Peng, Xu, Peng, Xiao, Han, Zhang, Kaipeng, Liu, Chris, Wen, Song, Guo, Ziyu, others.* arXiv preprint arXiv:2309.03905, 2023. [[abs](https://arxiv.org/abs/2309.03905)]
18. **OneLLM: One Framework to Align All Modalities with Language** *Han, Jiaming, Gong, Kaixiong, Zhang, Yiyuan, Wang, Jiaqi, Zhang, Kaipeng, Lin, Dahua, Qiao, Yu, Gao, Peng, Yue, Xiangyu.* arXiv preprint arXiv:2312.03700, 2023. [[abs](https://arxiv.org/abs/2312.03700)]
19. **Planting a seed of vision in large language model** *Ge, Yuying, Ge, Yixiao, Zeng, Ziyun, Wang, Xintao, Shan, Ying.* arXiv preprint arXiv:2307.08041, 2023. [[abs](https://arxiv.org/abs/2307.08041)]
20. **Seggpt: Segmenting everything in context** *Wang, Xinlong, Zhang, Xiaosong, Cao, Yue, Wang, Wen, Shen, Chunhua, Huang, Tiejun.* arXiv preprint arXiv:2304.03284, 2023. [[abs](https://arxiv.org/abs/2304.03284)]
21. **Audiogpt: Understanding and generating speech, music, sound, and talking head** *Huang, Rongjie, Li, Mingze, Yang, Dongchao, Shi, Jiatong, Chang, Xuankai, Ye, Zhenhui, Wu, Yuning, Hong, Zhiqing, Huang, Jiawei, Liu, Jinglin, others.* arXiv preprint arXiv:2304.12995, 2023. [[abs](https://arxiv.org/abs/2304.12995)]
22. **Holistic analysis of hallucination in gpt-4v (ision): Bias and interference challenges** *Cui, Chenhang, Zhou, Yiyang, Yang, Xinyu, Wu, Shirley, Zhang, Linjun, Zou, James, Yao, Huaxiu.* arXiv preprint arXiv:2311.03287, 2023. [[abs](https://arxiv.org/abs/2311.03287)]
23. **De-Diffusion Makes Text a Strong Cross-Modal Interface** *Wei, Chen, Liu, Chenxi, Qiao, Siyuan, Zhang, Zhishuai, Yuille, Alan, Yu, Jiahui.* arXiv preprint arXiv:2311.00618, 2023. [[abs](https://arxiv.org/abs/2311.00618)]
24. **Pandagpt: One model to instruction-follow them all** *Su, Yixuan, Lan, Tian, Li, Huayang, Xu, Jialu, Wang, Yan, Cai, Deng.* arXiv preprint arXiv:2305.16355, 2023. [[abs](https://arxiv.org/abs/2305.16355)]
25. **On evaluating adversarial robustness of large vision-language models** *Zhao, Yunqing, Pang, Tianyu, Du, Chao, Yang, Xiao, Li, Chongxuan, Cheung, Ngai-Man, Lin, Min.* arXiv preprint arXiv:2305.16934, 2023. [[abs](https://arxiv.org/abs/2305.16934)]
26. **Otter: A multi-modal model with in-context instruction tuning** *Li, Bo, Zhang, Yuanhan, Chen, Liangyu, Wang, Jinghao, Yang, Jingkang, Liu, Ziwei.* arXiv preprint arXiv:2305.03726, 2023. [[abs](https://arxiv.org/abs/2305.03726)]
27. **Minigpt-4: Enhancing vision-language understanding with advanced large language models** *Zhu, Deyao, Chen, Jun, Shen, Xiaoqian, Li, Xiang, Elhoseiny, Mohamed.* arXiv preprint arXiv:2304.10592, 2023. [[abs](https://arxiv.org/abs/2304.10592)]
28. **Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models** *Li, Junnan, Li, Dongxu, Savarese, Silvio, Hoi, Steven.* No journal, 2023.
29. **Visionllm: Large language model is also an open-ended decoder for vision-centric tasks** *Wang, Wenhai, Chen, Zhe, Chen, Xiaokang, Wu, Jiannan, Zhu, Xizhou, Zeng, Gang, Luo, Ping, Lu, Tong, Zhou, Jie, Qiao, Yu, others.* arXiv preprint arXiv:2305.11175, 2023. [[abs](https://arxiv.org/abs/2305.11175)]
30. **Grounding language models to images for multimodal inputs and outputs** *Koh, Jing Yu, Salakhutdinov, Ruslan, Fried, Daniel.* Presented at International Conference on Machine Learning, 2023. [Link](No URL)
31. **3D-GPT: Procedural 3D Modeling with Large Language Models** *Sun, Chunyi, Han, Junlin, Deng, Weijian, Wang, Xinlong, Qin, Zishan, Gould, Stephen.* arXiv preprint arXiv:2310.12945, 2023. [[abs](https://arxiv.org/abs/2310.12945)]
32. **Llama-adapter: Efficient fine-tuning of language models with zero-init attention** *Zhang, Renrui, Han, Jiaming, Zhou, Aojun, Hu, Xiangfei, Yan, Shilin, Lu, Pan, Li, Hongsheng, Gao, Peng, Qiao, Yu.* arXiv preprint arXiv:2303.16199, 2023. [[abs](https://arxiv.org/abs/2303.16199)]
33. **Visual instruction tuning** *Liu, Haotian, Li, Chunyuan, Wu, Qingyang, Lee, Yong Jae.* arXiv preprint arXiv:2304.08485, 2023. [[abs](https://arxiv.org/abs/2304.08485)]
34. **Multimodal-gpt: A vision and language model for dialogue with humans** *Gong, Tao, Lyu, Chengqi, Zhang, Shilong, Wang, Yudong, Zheng, Miao, Zhao, Qian, Liu, Kuikun, Zhang, Wenwei, Luo, Ping, Chen, Kai.* arXiv preprint arXiv:2305.04790, 2023. [[abs](https://arxiv.org/abs/2305.04790)]
35. **Towards language models that can see: Computer vision through the lens of natural language** *Berrios, William, Mittal, Gautam, Thrush, Tristan, Kiela, Douwe, Singh, Amanpreet.* arXiv preprint arXiv:2306.16410, 2023. [[abs](https://arxiv.org/abs/2306.16410)]
36. **Llama-adapter v2: Parameter-efficient visual instruction model** *Gao, Peng, Han, Jiaming, Zhang, Renrui, Lin, Ziyi, Geng, Shijie, Zhou, Aojun, Zhang, Wei, Lu, Pan, He, Conghui, Yue, Xiangyu, others.* arXiv preprint arXiv:2304.15010, 2023. [[abs](https://arxiv.org/abs/2304.15010)]
37. **Multimodal large language models: A survey** *Wu, Jiayang, Gan, Wensheng, Chen, Zefeng, Wan, Shicheng, Philip, S Yu.* Presented at 2023 IEEE International Conference on Big Data (BigData), 2023. [Link](No URL)
38. **Visual adversarial examples jailbreak aligned large language models** *Qi, Xiangyu, Huang, Kaixuan, Panda, Ashwinee, Wang, Mengdi, Mittal, Prateek.* Presented at The Second Workshop on New Frontiers in Adversarial Machine Learning, 2023. [Link](No URL)
39. **Introducing our Multimodal Models** *Bavishi, Rohan, Elsen, Erich, Hawthorne, Curtis, Nye, Maxwell, Odena, Augustus, Somani, Arushi,  Ta\cs.* arXiv 2023. [[abs](https://arxiv.org/abs/No eprint ID)]
40. **Gemini: a family of highly capable multimodal models** *Team, Gemini, Anil, Rohan, Borgeaud, Sebastian, Wu, Yonghui, Alayrac, Jean-Baptiste, Yu, Jiahui, Soricut, Radu, Schalkwyk, Johan, Dai, Andrew M, Hauth, Anja, others.* arXiv preprint arXiv:2312.11805, 2023. [[abs](https://arxiv.org/abs/2312.11805)]
41. **Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic** *Chen, Keqin, Zhang, Zhao, Zeng, Weili, Zhang, Richong, Zhu, Feng, Zhao, Rui.* arXiv preprint arXiv:2306.15195, 2023. [[abs](https://arxiv.org/abs/2306.15195)]
42. **OtterHD: A High-Resolution Multi-modality Model** *Li, Bo, Zhang, Peiyuan, Yang, Jingkang, Zhang, Yuanhan, Pu, Fanyi, Liu, Ziwei.* arXiv preprint arXiv:2311.04219, 2023. [[abs](https://arxiv.org/abs/2311.04219)]
43. **Minigpt-5: Interleaved vision-and-language generation via generative vokens** *Zheng, Kaizhi, He, Xuehai, Wang, Xin Eric.* arXiv preprint arXiv:2310.02239, 2023. [[abs](https://arxiv.org/abs/2310.02239)]
44. **A Simple LLM Framework for Long-Range Video Question-Answering** *Zhang, Ce, Lu, Taixi, Islam, Md Mohaiminul, Wang, Ziyang, Yu, Shoubin, Bansal, Mohit, Bertasius, Gedas.* arXiv preprint arXiv:2312.17235, 2023. [[abs](https://arxiv.org/abs/2312.17235)]
45. **Camel: Communicative agents for" mind" exploration of large scale language model society** *Li, Guohao, Hammoud, Hasan Abed Al Kader, Itani, Hani, Khizbullin, Dmitrii, Ghanem, Bernard.* Presented at No conference, 2023. [Link](No URL)
46. **Video-llama: An instruction-tuned audio-visual language model for video understanding** *Zhang, Hang, Li, Xin, Bing, Lidong.* arXiv preprint arXiv:2306.02858, 2023. [[abs](https://arxiv.org/abs/2306.02858)]
47. **Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning** *Zhao, Bingchen, Tu, Haoqin, Wei, Chen, Mei, Jieru, Xie, Cihang.* arXiv preprint arXiv:2312.11420, 2023. [[abs](https://arxiv.org/abs/2312.11420)]
48. **SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models** *Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, Jiaming Han, Siyuan Huang, Yichi Zhang, Xuming He, Hongsheng Li, Yu Jiao Qiao.* ArXiv, 2023.
49. **Gpt4tools: Teaching large language model to use tools via self-instruction** *Yang, Rui, Song, Lin, Li, Yanwei, Zhao, Sijie, Ge, Yixiao, Li, Xiu, Shan, Ying.* arXiv preprint arXiv:2305.18752, 2023. [[abs](https://arxiv.org/abs/2305.18752)]
50. **Generative pretraining in multimodality** *Sun, Quan, Yu, Qiying, Cui, Yufeng, Zhang, Fan, Zhang, Xiaosong, Wang, Yueze, Gao, Hongcheng, Liu, Jingjing, Huang, Tiejun, Wang, Xinlong.* arXiv preprint arXiv:2307.05222, 2023. [[abs](https://arxiv.org/abs/2307.05222)]
51. **Agents: An open-source framework for autonomous language agents** *Zhou, Wangchunshu, Jiang, Yuchen Eleanor, Li, Long, Wu, Jialong, Wang, Tiannan, Qiu, Shi, Zhang, Jintian, Chen, Jing, Wu, Ruipu, Wang, Shuai, others.* arXiv preprint arXiv:2309.07870, 2023. [[abs](https://arxiv.org/abs/2309.07870)]
52. **LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment** *Zhu, Bin, Lin, Bin, Ning, Munan, Yan, Yang, Cui, Jiaxi, Wang, HongFa, Pang, Yatian, Jiang, Wenhao, Zhang, Junwu, Li, Zongwei, others.* arXiv preprint arXiv:2310.01852, 2023. [[abs](https://arxiv.org/abs/2310.01852)]
53. **Sight Beyond Text: Multi-Modal Training Enhances LLMs in Truthfulness and Ethics** *Tu, Haoqin, Zhao, Bingchen, Wei, Chen, Xie, Cihang.* Presented at NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023. [Link](No URL)
54. **How Many Unicorns Are in This Image? A Safety Evaluation Benchmark for Vision LLMs** *Tu, Haoqin, Cui, Chenhang, Wang, Zijun, Zhou, Yiyang, Zhao, Bingchen, Han, Junlin, Zhou, Wangchunshu, Yao, Huaxiu, Xie, Cihang.* arXiv preprint arXiv:2311.16101, 2023. [[abs](https://arxiv.org/abs/2311.16101)]
55. **Imagebind: One embedding space to bind them all** *Girdhar, Rohit, El-Nouby, Alaaeldin, Liu, Zhuang, Singh, Mannat, Alwala, Kalyan Vasudev, Joulin, Armand, Misra, Ishan.* Presented at Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. [Link](No URL)
56. **Mobilevlm: A fast, reproducible and strong vision language assistant for mobile devices** *Chu, Xiangxiang, Qiao, Limeng, Lin, Xinyang, Xu, Shuang, Yang, Yang, Hu, Yiming, Wei, Fei, Zhang, Xinyu, Zhang, Bo, Wei, Xiaolin, others.* arXiv preprint arXiv:2312.16886, 2023. [[abs](https://arxiv.org/abs/2312.16886)]
57. **Llava-interactive: An all-in-one demo for image chat, segmentation, generation and editing** *Chen, Wei-Ge, Spiridonova, Irina, Yang, Jianwei, Gao, Jianfeng, Li, Chunyuan.* arXiv preprint arXiv:2311.00571, 2023. [[abs](https://arxiv.org/abs/2311.00571)]
58. **What Makes for Good Visual Tokenizers for Large Language Models?** *Wang, Guangzhi, Ge, Yixiao, Ding, Xiaohan, Kankanhalli, Mohan, Shan, Ying.* arXiv preprint arXiv:2305.12223, 2023. [[abs](https://arxiv.org/abs/2305.12223)]
59. **Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi** *Yue, Xiang, Ni, Yuansheng, Zhang, Kai, Zheng, Tianyu, Liu, Ruoqi, Zhang, Ge, Stevens, Samuel, Jiang, Dongfu, Ren, Weiming, Sun, Yuxuan, others.* arXiv preprint arXiv:2311.16502, 2023. [[abs](https://arxiv.org/abs/2311.16502)]
60. **Mementos: A comprehensive benchmark for multimodal large language model reasoning over image sequences** *Wang, Xiyao, Zhou, Yuhang, Liu, Xiaoyu, Lu, Hongjin, Xu, Yuancheng, He, Feihong, Yoon, Jaehong, Lu, Taixi, Bertasius, Gedas, Bansal, Mohit, others.* arXiv preprint arXiv:2401.10529, 2024. [[abs](https://arxiv.org/abs/2401.10529)]
61. **Muffin or Chihuahua? Challenging Large Vision-Language Models with Multipanel VQA** *Fan, Yue, Gu, Jing, Zhou, Kaiwen, Yan, Qianqi, Jiang, Shan, Kuo, Ching-Chen, Guan, Xinze, Wang, Xin Eric.* arXiv preprint arXiv:2401.15847, 2024. [[abs](https://arxiv.org/abs/2401.15847)]
62. **Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs** *Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, Saining Xie.* ArXiv, 2024.
63. **LEGO: Language Enhanced Multi-modal Grounding Model** *Li, Zhaowei, Xu, Qi, Zhang, Dong, Song, Hang, Cai, Yiqing, Qi, Qi, Zhou, Ran, Pan, Junting, Li, Zefeng, Vu, Van Tu, others.* arXiv preprint arXiv:2401.06071, 2024. [[abs](https://arxiv.org/abs/2401.06071)]
64. **Incorporating Visual Experts to Resolve the Information Loss in Multimodal Large Language Models** *He, Xin, Wei, Longhui, Xie, Lingxi, Tian, Qi.* arXiv preprint arXiv:2401.03105, 2024. [[abs](https://arxiv.org/abs/2401.03105)]

### 2.2 Reasoning
1. **Chain-of-Thought Prompting Elicits Reasoning in Large Language Models**. *Jason Wei* et al. NeurIPS 2022. [[paper](https://arxiv.org/abs/2201.11903)]
4. **Decomposed Prompting: A Modular Approach for Solving Complex Tasks**. *Tushar Khot* et al. arXiv 2022. [[paper](https://arxiv.org/abs/2210.02406)]
5. **Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs**. *Maarten Sap* et al. EMNLP 2022. [[paper](https://arxiv.org/abs/2210.13312)]
6. **Inner Monologue: Embodied Reasoning through Planning with Language Models**. *Wenlong Huang* et al. CoRL 2022. [[paper](https://arxiv.org/abs/2207.05608)]
7. **Survey of Hallucination in Natural Language Generation**. *Ziwei Ji* et al. ACM Computing Surveys 2022. [[paper](https://arxiv.org/abs/2202.03629)]
8. **ReAct: Synergizing Reasoning and Acting in Language Models**. *Shunyu Yao* et al. ICLR 2023. [[paper](https://arxiv.org/abs/2210.03629)]
3. **Complexity-Based Prompting for Multi-Step Reasoning**. *Yao Fu* et al. ICLR 2023. [[paper](https://arxiv.org/abs/2210.00720)]
7. **Towards Reasoning in Large Language Models: A Survey**. *Jie Huang* et al. ACL Findings 2023. [[paper](https://arxiv.org/abs/2212.10403)]
9. **Least-to-Most Prompting Enables Complex Reasoning in Large Language Models**. *Denny Zhou* et al. ICLR 2023. [[paper](https://arxiv.org/abs/2205.10625)]
10. **ProgPrompt: Generating Situated Robot Task Plans using Large Language Models**. *Ishika Singh* et al. ICRA 2023. [[paper](https://arxiv.org/abs/2209.11302)]
11. **Reasoning with Language Model is Planning with World Model**. *Shibo Hao* et al. EMNLP 2023. [[paper](https://arxiv.org/abs/2305.14992)]
12. **Evaluating Object Hallucination in Large Vision-Language Models**. *Yifan Li* et al. EMNLP 2023. [[paper](https://arxiv.org/abs/2305.10355)]
13. **Tree of Thoughts: Deliberate Problem Solving with Large Language Models**. *Shunyu Yao* et al. NeurIPS 2023. [[paper](https://arxiv.org/abs/2305.10601)]
14. **Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents**. *Zihao Wang* et al. NeurIPS 2023. [[paper](https://arxiv.org/abs/2302.01560)]
15. **LLM+P: Empowering Large Language Models with Optimal Planning Proficiency**. *Bo Liu* et al. arXiv 2023. [[paper](https://arxiv.org/abs/2304.11477)]
16. **Language Models, Agent Models, and World Models: The LAW for Machine Reasoning and Planning**. *Zhiting Hu* et al. arXiv 2023. [[paper](https://arxiv.org/abs/2312.05230)]
17. **MMToM-QA: Multimodal Theory of Mind Question Answering**. *Chuanyang Jin* et al. arXiv 2024. [[paper](https://arxiv.org/abs/2401.08743)]
18. **Graph of Thoughts: Solving Elaborate Problems with Large Language Models**. *Maciej Besta* et al. AAAI 2024. [[paper](https://arxiv.org/abs/2308.09687)]

### 2.3 Memory
1. **Dense Passage Retrieval for Open-Domain Question Answering**. *Vladimir Karpukhin* et al. EMNLP 2020. [[paper](https://arxiv.org/abs/2004.04906)]
2. **Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks**. *Patrick Lewis* et al. NeurIPS 2020. [[paper](https://arxiv.org/abs/2005.11401)]
3. **REALM: Retrieval-Augmented Language Model Pre-Training**. *Kelvin Guu* et al. ICML 2020. [[paper](https://arxiv.org/abs/2002.08909)]
4. **Retrieval Augmentation Reduces Hallucination in Conversation**. *Kurt Shuster* et al. EMNLP Findings 2021. [[paper](https://arxiv.org/abs/2104.07567)]
5. **Improving Language Models by Retrieving from Trillions of Tokens**. *Sebastian Borgeaud* et al. ICML 2022. [[paper](https://arxiv.org/abs/2112.04426)]
6. **FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness**. *Tri Dao* et al. NeurIPS 2022. [[paper](https://arxiv.org/abs/2205.14135)]
7. **Generative Agents: Interactive Simulacra of Human Behavior**. *Joon Sung Park* et al. UIST 2023. [[paper](https://arxiv.org/abs/2304.03442)]
8. **Cognitive Architectures for Language Agents**. *Theodore R. Sumers* et al. TMLR 2024. [[paper](https://arxiv.org/abs/2309.02427)]

### 2.4 Metacognition
1. **Evolving Self-supervised Neural Networks: Autonomous Intelligence from Evolved Self-teaching**
   *Nam Le.* arXiv, 2019. [\[eprint\]](https://arxiv.org/abs/1906.08865)

2. **Self-Instruct: Aligning Language Models with Self-Generated Instructions**
   *Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, Hannaneh Hajishirzi.* arXiv, 2022. [\[eprint\]](https://arxiv.org/abs/2212.10560)

3. **ReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent**
   *Renat Aksitov, Sobhan Miryoosefi, Zonglin Li, Daliang Li, Sheila Babayan, Kavya Kopparapu, Zachary Fisher, Ruiqi Guo, Sushant Prakash, Pranesh Srinivasan, Manzil Zaheer, Felix Yu, Sanjiv Kumar.* arXiv, 2023. [\[eprint\]](https://arxiv.org/abs/2312.10003)

4. **Wizardlm: Empowering large language models to follow complex instructions**
   *Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Daxin Jiang.* arXiv, 2023. [\[abs\]](https://arxiv.org/abs/2304.12244)

## 3. AGI Interface: Connecting the World with AGI
### 3.1 Interfaces to Digital World
### 3.2 Interfaces to Physical World
### 3.3 Interfaces to Intelligence

#### 3.3.2 Interfaces to Humans

1. **Guidelines for Human-AI Interaction**
   *Saleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira Nushi, Penny Collisson, Jina Suh, Shamsi Iqbal, Paul N. Bennett, Kori Inkpen, Jaime Teevan, Ruth Kikin-Gil, Eric Horvitz*. CHI 2019. [[paper](https://dl.acm.org/doi/10.1145/3290605.3300233)]

2. **Design Principles for Generative AI Applications**
   *Justin D. Weisz, Jessica He, Michael Muller, Gabriela Hoefer, Rachel Miles, Werner Geyer*. CHI 2024. [[paper](http://arxiv.org/abs/2401.14484)]

3. **Graphologue: Exploring Large Language Model Responses with Interactive Diagrams**
   *Peiling Jiang, Jude Rayan, Steven P. Dow, Haijun Xia*. UIST 2023. [[paper](https://dl.acm.org/doi/10.1145/3586183.3606737)]

4. **Sensecape: Enabling Multilevel Exploration and Sensemaking with Large Language Models**
   *Sangho Suh, Bryan Min, Srishti Palani, Haijun Xia*. UIST 2023. [[paper](https://dl.acm.org/doi/10.1145/3586183.3606756)]

5. **Supporting Sensemaking of Large Language Model Outputs at Scale**
   *Katy Ilonka Gero, Chelse Swoopes, Ziwei Gu, Jonathan K. Kummerfeld, Elena L. Glassman*. CHI 2024. [[paper](https://arxiv.org/abs/2401.13726)]

6. **Luminate: Structured Generation and Exploration of Design Space with Large Language Models for Human-AI Co-Creation**
   *Sangho Suh, Meng Chen, Bryan Min, Toby Jia-Jun Li, Haijun Xia*. CHI 2024. [[Paper](http://arxiv.org/abs/2310.12953)]

7. **AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts**
   *Tongshuang Wu, Michael Terry, Carrie Jun Cai*. CHI 2022. [[paper](https://dl.acm.org/doi/10.1145/3491102.3517582)]

8. **Promptify: Text-to-Image Generation through Interactive Prompt Exploration with Large Language Models**
   *Stephen Brade, Bryan Wang, Mauricio Sousa, Sageev Oore, Tovi Grossman*. CHI 2023. [[paper](https://dl.acm.org/doi/10.1145/3586183.3606725)]

9. **ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing**
   *Ian Arawjo, Chelse Swoopes, Priyan Vaithilingam, Martin Wattenberg, Elena Glassman*. CHI 2024. [[paper](https://doi.org/10.48550/arXiv.2309.09128)]

10. **CoPrompt: Supporting Prompt Sharing and Referring in Collaborative Natural Language Programming**
    *Li Feng, Ryan Yen, Yuzhe You, Mingming Fan, Jian Zhao, Zhicong Lu*. CHI 2024. [[paper](http://arxiv.org/abs/2310.09235)]

11. **Generating Automatic Feedback on UI Mockups with Large Language Models**
    *Peitong Duan, Jeremy Warner, Yang Li, Bj√∂rn Hartmann*. CHI 2024. [[paper](http://arxiv.org/abs/2403.13139)]

12. **Rambler: Supporting Writing With Speech via LLM-Assisted Gist Manipulation**
    *Susan Lin, Jeremy Warner, J. D. Zamfirescu-Pereira, Matthew G. Lee, Sauhard Jain, Michael Xuelin Huang, Piyawat Lertvittayakumjorn, Shanqing Cai, Shumin Zhai, Bj√∂rn Hartmann, Can Liu*. CHI 2024. [[paper](http://arxiv.org/abs/2401.10838)]

13. **Embedding Large Language Models into Extended Reality: Opportunities and Challenges for Inclusion, Engagement, and Privacy**
    *Efe Bozkir, S√ºleyman √ñzdel, Ka Hei Carrie Lau, Mengdi Wang, Hong Gao, Enkelejda Kasneci*. arXiv 2024. [[paper](http://arxiv.org/abs/2402.03907)]

14. **GenAssist: Making Image Generation Accessible**
    *Mina Huh, Yi-Hao Peng, Amy Pavel*. UIST 2023. [[paper](https://dl.acm.org/doi/10.1145/3586183.3606735)]

15. **‚ÄúThe less I type, the better‚Äù: How AI Language Models can Enhance or Impede Communication for AAC Users**
    *Stephanie Valencia, Richard Cave, Krystal Kallarackal, Katie Seaver, Michael Terry, Shaun K. Kane*. CHI 2023. [[paper](https://dl.acm.org/doi/10.1145/3544548.3581560)]

16. **Re-examining Whether, Why, and How Human-AI Interaction Is Uniquely Difficult to Design**
    *Qian Yang, Aaron Steinfeld, Carolyn Ros√©, John Zimmerman*. CHI 2020. [[paper](https://dl.acm.org/doi/10.1145/3313831.3376301)]

## 4. AGI Systems: Implementing the Mechanism of AGI
### 4.1 System Challenges
### 4.2 Scalable Model Architectures
### 4.3 Large-scale Training
### 4.4 Inference Techniques
### 4.5 Cost and Efficiency
### 4.6 Computing Platforms
### 4.7 The Future of AGI Systems

## 5. AGI Alignment: Reconciling Needs with AGI
### 5.1 Expectations of AGI Alignment

1. **Human Compatible: Artificial Intelligence and the Problem of Control**
   *Stuart Russell*. Viking, 2019.
2. **Artificial Intelligence, Values and Alignment**
   *Iason Gabriel*. Minds and Machines, 2020. [[paper](http://arxiv.org/abs/2001.09768)]
3. **Alignment of Language Agents**
   *Zachary Kenton, Tom Everitt, Laura Weidinger, Iason Gabriel, Vladimir Mikulik, Geoffrey Irving*. arXiv, 2021. [[Paper](http://arxiv.org/abs/2103.14659)]
4. **The Value Learning Problem** 
   *Nate Soares*. Machine Intelligence Research Institute Technical Report [[paper](https://intelligence.org/files/ValueLearningProblem.pdf)]
5. **Concrete Problems in AI Safety**
   *Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, Dan Man√©*. arXiv, 2016. [[paper](http://arxiv.org/abs/1606.06565)]
6. **Ethical and social risks of harm from Language Models**
   *Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, Zac Kenton, Sasha Brown, Will Hawkins, Tom Stepleton, Courtney Biles, Abeba Birhane, Julia Haas, Laura Rimell, Lisa Anne Hendricks, William Isaac, Sean Legassick, Geoffrey Irving, Iason Gabriel*. arXiv, 2021. [[paper](http://arxiv.org/abs/2112.04359)]
7. **On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?**
   *Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, Shmargaret Shmitchell*. FAccT 2021. [[paper](https://dl.acm.org/doi/10.1145/3442188.3445922)]
8. **The global landscape of AI ethics guidelines**
   *Anna Jobin, Marcello Ienca, Effy Vayena*. Nature Machine Intelligence, 2019. [[paper](https://www.nature.com/articles/s42256-019-0088-2)]
9. **Persistent Anti-Muslim Bias in Large Language Models**
   *Abubakar Abid, Maheen Farooqi, James Zou*. AIES, 2021. [[paper](http://arxiv.org/abs/2101.05783)]
10. **Toward Gender-Inclusive Coreference Resolution**
      *Yang Trista Cao, Hal Daum√© III*. ACL, 2020. [[paper](https://aclanthology.org/2020.acl-main.418)]
11. **The Social Impact of Natural Language Processing**
    *Dirk Hovy, Shannon L. Spruit*. ACL 2016. [[paper](https://aclanthology.org/P16-2096)]
12. **TruthfulQA: Measuring How Models Mimic Human Falsehoods**
    *Stephanie Lin, Jacob Hilton, Owain Evans*. ACL 2022. [[paper](https://aclanthology.org/2022.acl-long.229)]
13. **The Radicalization Risks of GPT-3 and Advanced Neural Language Models**
    *Kris McGuffie, Alex Newhouse*. arXiv, 2020. [[paper](http://arxiv.org/abs/2009.06807)]
14. **AI Transparency in the Age of LLMs: A Human-Centered Research Roadmap**
    *Q. Vera Liao, Jennifer Wortman Vaughan*. arXiv 2023. [[paper](http://arxiv.org/abs/2306.01941)]
15. **Beyond Expertise and Roles: A Framework to Characterize the Stakeholders of Interpretable Machine Learning and their Needs**
    *Harini Suresh, Steven R. Gomez, Kevin K. Nam, Arvind Satyanarayan*. CHI 2021. [[paper](https://dl.acm.org/doi/10.1145/3411764.3445088)]
16. **Identifying and Mitigating the Security Risks of Generative AI**
    *Clark Barrett, Brad Boyd, Elie Burzstein, Nicholas Carlini, Brad Chen, Jihye Choi, Amrita Roy Chowdhury, Mihai Christodorescu, Anupam Datta, Soheil Feizi, Kathleen Fisher, Tatsunori Hashimoto, Dan Hendrycks, Somesh Jha, Daniel Kang, Florian Kerschbaum, Eric Mitchell, John Mitchell, Zulfikar Ramzan, Khawaja Shams, Dawn Song, Ankur Taly, Diyi Yang*. arXiv, 2023. [[paper](http://arxiv.org/abs/2308.14840)]
17. **LLM Agents can Autonomously Hack Websites**
    *Richard Fang, Rohan Bindu, Akul Gupta, Qiusi Zhan, Daniel Kang*. arXiv, 2024. [[paper](http://arxiv.org/abs/2402.06664)]
18. **Deepfakes, Phrenology, Surveillance, and More! A Taxonomy of AI Privacy Risks**
    *Hao-Ping Lee, Yu-Ju Yang, Thomas Serban von Davier, Jodi Forlizzi, Sauvik Das*. CHI 2024. [[paper](http://arxiv.org/abs/2310.07879)]
19. **Privacy in the Age of AI**
    *Sauvik Das, Hao-Ping (Hank) Lee, Jodi Forlizzi*. Communications of the ACM, 2023. [[paper](https://dl.acm.org/doi/10.1145/3625254)]

### 5.2 AGI Alignment Classifications
### 5.3 How to Implement: Solutions for Alignment

## 6. Approach AGI Responsibly
### 6.1 AI Levels: Charting the Evolution of Artificial Intelligence
#### 6.1.1 AGI Levels
#### 6.1.2 Constraints and Challenges of Ultimate AGI
#### 6.1.3 How Do We Get to the Next Level of AGI?
### 6.2 AGI Evaluation
#### 6.2.1 What Do We Expect from AGI Evaluations
#### 6.2.2 Current Evaluation Frameworks and Limitations
### 6.3 Potential Ways to Future AGI

## 7. Case Studies
### 7.1 AI for Science Discovery and Research
### 7.2 Generative Visual Intelligence
1. **Deep Unsupervised Learning using Nonequilibrium Thermodynamics**. *Jascha Sohl-Dickstein* et al. ICML 2015. [[paper](https://arxiv.org/abs/1503.03585)]
2. **Generative Modeling by Estimating Gradients of the Data Distribution**. *Yang Song* et al. NeurIPS 2019. [[paper](https://arxiv.org/abs/1907.05600)]
3. **Denoising Diffusion Probabilistic Models**. *Jonathan Ho* et al. NeurIPS 2020. [[paper](https://arxiv.org/abs/2006.11239)]
4. **Score-Based Generative Modeling through Stochastic Differential Equations**. *Yang Song* et al. ICLR 2021. [[paper](https://arxiv.org/abs/2011.13456)]
5. **GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models**. *Alex Nichol* et al. arXiv 2021. [[paper](https://arxiv.org/abs/2112.10741)]
6. **SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations**. *Chenlin Meng* et al. ICLR 2022. [[paper](https://arxiv.org/abs/2108.01073)]
7. **Video Diffusion Models**. *Jonathan Ho* et al. NeurIPS 2022. [[paper](https://arxiv.org/abs/2204.03458)]
8. **Hierarchical Text-Conditional Image Generation with CLIP Latents**. *Aditya Ramesh* et al. arXiv 2022. [[paper](https://arxiv.org/abs/2204.06125)]
9. **Classifier-Free Diffusion Guidance**. *Jonathan Ho* et al. arXiv 2022. [[paper](https://arxiv.org/abs/2207.12598)]
10. **Palette: Image-to-Image Diffusion Models**. *Chitwan Saharia* et al. SIGGRAPH 2022. [[paper](https://arxiv.org/abs/2111.05826)]
11. **High-Resolution Image Synthesis with Latent Diffusion Models**. *Robin Rombach* et al. CVPR 2022. [[paper](https://arxiv.org/abs/2112.10752)]
12. **Adding Conditional Control to Text-to-Image Diffusion Models**. *Lvmin Zhang* et al. ICCV 2023. [[paper](https://arxiv.org/abs/2302.05543)]
13. **Scalable Diffusion Models with Transformers**. *William Peebles* et al. ICCV 2023. [[paper](https://arxiv.org/abs/2212.09748)]
14. **Sequential Modeling Enables Scalable Learning for Large Vision Models**. *Yutong Bai* et al. arXiv 2023. [[paper](https://arxiv.org/abs/2312.00785)]
15. **Video Generation Models as World Simulators**. *Tim Brooks* et al. OpenAI 2024. [[paper](https://openai.com/research/video-generation-models-as-world-simulators)]

### 7.3 World Models
### 7.4 Decentralized LLM
### 7.5 AI for Coding
### 7.6 AI for Robotics in Real World Applications
### 7.7 Human-AI Collaboration

## 8. Conclusion


<!-- ### Agent & Tool:
- [Tool learning with foundation models](https://arxiv.org/pdf/2304.08354.pdf)
- [A Survey on Large Language Model based Autonomous Agents](https://arxiv.org/pdf/2308.11432.pdf)
- [The Rise and Potential of Large Language Model Based Agents: A Survey](https://arxiv.org/pdf/2309.07864.pdf)
- [Cognitive Architectures for Language Agents](https://arxiv.org/pdf/2309.02427.pdf)

### (Multi-Modal) LLM / Foundation Model:
#### LLM
- [A Survey of Large Language Models](https://arxiv.org/pdf/2303.18223.pdf)
- [A Survey on In-context Learning](https://arxiv.org/pdf/2301.00234.pdf)
- [Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond](https://arxiv.org/pdf/2304.13712.pdf)

#### Multi-Modal LLM
- [A Survey on Multimodal Large Language Models](https://arxiv.org/pdf/2306.13549.pdf)

#### General Foundation Model
- [On the Opportunities and Risks of Foundation Models](https://arxiv.org/pdf/2108.07258.pdf)
- [A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT](https://arxiv.org/pdf/2302.09419.pdf)

### Embodied AI:
#### AGI
- [One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era](https://arxiv.org/pdf/2304.06488.pdf)
- [Sparks of Artificial General Intelligence: Early experiments with GPT-4](https://arxiv.org/pdf/2303.12712.pdf)
- [Towards AGI in Computer Vision: Lessons Learned from GPT and Large Language Models](https://arxiv.org/pdf/2306.08641.pdf)
- [A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT](https://arxiv.org/pdf/2303.04226.pdf)

### Github:
- [LLM-Agent-Paper-List](https://github.com/WooooDyy/LLM-Agent-Paper-List) (Agent)
- [Awesome-AI-Agents](https://github.com/e2b-dev/awesome-ai-agents) (Agent)
- [Awesome-Multimodal-Large-Language-Models](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models) (Multi-modal LLM) -->
