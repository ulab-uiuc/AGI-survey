# Awesom-AGI Survey Papers

Must-read Papers on Artifical General Intelligence with foundation models.

---

# üìúContent

- [Awesom-AGI Survey Papers](#awesom-agi-survey-papers)
- [üìúContent](#content)
  - [1. Introduction](#1-introduction)
  - [2. AGI Internal: Unveiling the Mind of AGI](#2-agi-internal-unveiling-the-mind-of-agi)
    - [2.1 Perception](#21-perception)
    - [2.2 Reasoning](#22-reasoning)
    - [2.3 Memory](#23-memory)
    - [2.4 Metacognition](#24-metacognition)
  - [3. AGI Interface: Connecting the World with AGI](#3-agi-interface-connecting-the-world-with-agi)
    - [3.1 Interfaces to Digital World](#31-interfaces-to-digital-world)
    - [3.2 Interfaces to Physical World](#32-interfaces-to-physical-world)
    - [3.3 Interfaces to Intelligence](#33-interfaces-to-intelligence)
      - [3.3.1 Interfaces to AI Agents](#331-interfaces-to-ai-agents)
      - [3.3.2 Interfaces to Humans](#332-interfaces-to-humans)
  - [4. AGI Systems: Implementing the Mechanism of AGI](#4-agi-systems-implementing-the-mechanism-of-agi)
    - [4.2 Scalable Model Architectures](#42-scalable-model-architectures)
    - [4.3 Large-scale Training](#43-large-scale-training)
    - [4.4 Inference Techniques](#44-inference-techniques)
    - [4.5 Cost and Efficiency](#45-cost-and-efficiency)
    - [4.6 Computing Platforms](#46-computing-platforms)
  - [5. AGI Alignment: Reconciling Needs with AGI](#5-agi-alignment-reconciling-needs-with-agi)
    - [5.1 Expectations of AGI Alignment](#51-expectations-of-agi-alignment)
    - [5.2 Current Alignment Techniques](#52-current-alignment-techniques)
    - [5.3 How to approach AGI Alignments](#53-how-to-approach-agi-alignments)
  - [6. Approach AGI Responsibly](#6-approach-agi-responsibly)
    - [6.1 AI Levels: Charting the Evolution of Artificial Intelligence](#61-ai-levels-charting-the-evolution-of-artificial-intelligence)
      - [6.1.1 AGI Levels](#611-agi-levels)
      - [6.1.2 Constraints and Challenges of Ultimate AGI](#612-constraints-and-challenges-of-ultimate-agi)
      - [6.1.3 How Do We Get to the Next Level of AGI?](#613-how-do-we-get-to-the-next-level-of-agi)
    - [6.2 AGI Evaluation](#62-agi-evaluation)
      - [6.2.1 What Do We Expect from AGI Evaluations](#621-what-do-we-expect-from-agi-evaluations)
      - [6.2.2 Current Evaluation Frameworks and Limitations](#622-current-evaluation-frameworks-and-limitations)
    - [6.3 Potential Ways to Future AGI](#63-potential-ways-to-future-agi)
  - [7. Case Studies](#7-case-studies)
    - [7.1 AI for Science Discovery and Research](#71-ai-for-science-discovery-and-research)
    - [7.2 Generative Visual Intelligence](#72-generative-visual-intelligence)
    - [7.3 World Models](#73-world-models)
    - [7.4 Decentralized LLM](#74-decentralized-llm)
    - [7.5 AI for Coding](#75-ai-for-coding)
    - [7.6 AI for Robotics in Real World Applications](#76-ai-for-robotics-in-real-world-applications)
    - [7.7 Human-AI Collaboration](#77-human-ai-collaboration)
  - [8. Conclusion](#8-conclusion)


## 1. Introduction

## 2. AGI Internal: Unveiling the Mind of AGI
### 2.1 Perception

1. **Flamingo: a visual language model for few-shot learning** *Alayrac, Jean-Baptiste, Donahue, Jeff, Luc, Pauline, Miech, Antoine, Barr, Iain, Hasson, Yana, Lenc, Karel, Mensch, Arthur, Millican, Katherine, Reynolds, Malcolm, others.* No journal, 2022.
2. **Otter: A multi-modal model with in-context instruction tuning** *Li, Bo, Zhang, Yuanhan, Chen, Liangyu, Wang, Jinghao, Yang, Jingkang, Liu, Ziwei.* arXiv preprint arXiv:2305.03726, 2023. [[abs](https://arxiv.org/abs/2305.03726)]
3. **Videochat: Chat-centric video understanding** *Li, KunChang, He, Yinan, Wang, Yi, Li, Yizhuo, Wang, Wenhai, Luo, Ping, Wang, Yali, Wang, Limin, Qiao, Yu.* arXiv preprint arXiv:2305.06355, 2023. [[abs](https://arxiv.org/abs/2305.06355)]
4. **mplug-owl: Modularization empowers large language models with multimodality** *Ye, Qinghao, Xu, Haiyang, Xu, Guohai, Ye, Jiabo, Yan, Ming, Zhou, Yiyang, Wang, Junyang, Hu, Anwen, Shi, Pengcheng, Shi, Yaya, others.* arXiv preprint arXiv:2304.14178, 2023. [[abs](https://arxiv.org/abs/2304.14178)]
5. **A Survey on Multimodal Large Language Models** *Yin, Shukang, Fu, Chaoyou, Zhao, Sirui, Li, Ke, Sun, Xing, Xu, Tong, Chen, Enhong.* arXiv preprint arXiv:2306.13549, 2023. [[abs](https://arxiv.org/abs/2306.13549)]
6. **OneLLM: One Framework to Align All Modalities with Language** *Han, Jiaming, Gong, Kaixiong, Zhang, Yiyuan, Wang, Jiaqi, Zhang, Kaipeng, Lin, Dahua, Qiao, Yu, Gao, Peng, Yue, Xiangyu.* arXiv preprint arXiv:2312.03700, 2023. [[abs](https://arxiv.org/abs/2312.03700)]
7. **Audiogpt: Understanding and generating speech, music, sound, and talking head** *Huang, Rongjie, Li, Mingze, Yang, Dongchao, Shi, Jiatong, Chang, Xuankai, Ye, Zhenhui, Wu, Yuning, Hong, Zhiqing, Huang, Jiawei, Liu, Jinglin, others.* arXiv preprint arXiv:2304.12995, 2023. [[abs](https://arxiv.org/abs/2304.12995)]
8. **Holistic analysis of hallucination in gpt-4v (ision): Bias and interference challenges** *Cui, Chenhang, Zhou, Yiyang, Yang, Xinyu, Wu, Shirley, Zhang, Linjun, Zou, James, Yao, Huaxiu.* arXiv preprint arXiv:2311.03287, 2023. [[abs](https://arxiv.org/abs/2311.03287)]
9. **Pandagpt: One model to instruction-follow them all** *Su, Yixuan, Lan, Tian, Li, Huayang, Xu, Jialu, Wang, Yan, Cai, Deng.* arXiv preprint arXiv:2305.16355, 2023. [[abs](https://arxiv.org/abs/2305.16355)]
10. **On evaluating adversarial robustness of large vision-language models** *Zhao, Yunqing, Pang, Tianyu, Du, Chao, Yang, Xiao, Li, Chongxuan, Cheung, Ngai-Man, Lin, Min.* arXiv preprint arXiv:2305.16934, 2023. [[abs](https://arxiv.org/abs/2305.16934)]
11. **Minigpt-4: Enhancing vision-language understanding with advanced large language models** *Zhu, Deyao, Chen, Jun, Shen, Xiaoqian, Li, Xiang, Elhoseiny, Mohamed.* arXiv preprint arXiv:2304.10592, 2023. [[abs](https://arxiv.org/abs/2304.10592)]
12. **Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models** *Li, Junnan, Li, Dongxu, Savarese, Silvio, Hoi, Steven.* No journal, 2023.
13. **Visionllm: Large language model is also an open-ended decoder for vision-centric tasks** *Wang, Wenhai, Chen, Zhe, Chen, Xiaokang, Wu, Jiannan, Zhu, Xizhou, Zeng, Gang, Luo, Ping, Lu, Tong, Zhou, Jie, Qiao, Yu, others.* arXiv preprint arXiv:2305.11175, 2023. [[abs](https://arxiv.org/abs/2305.11175)]
14. **Llama-adapter: Efficient fine-tuning of language models with zero-init attention** *Zhang, Renrui, Han, Jiaming, Zhou, Aojun, Hu, Xiangfei, Yan, Shilin, Lu, Pan, Li, Hongsheng, Gao, Peng, Qiao, Yu.* arXiv preprint arXiv:2303.16199, 2023. [[abs](https://arxiv.org/abs/2303.16199)]
15. **Visual instruction tuning** *Liu, Haotian, Li, Chunyuan, Wu, Qingyang, Lee, Yong Jae.* arXiv preprint arXiv:2304.08485, 2023. [[abs](https://arxiv.org/abs/2304.08485)]
16. **Multimodal-gpt: A vision and language model for dialogue with humans** *Gong, Tao, Lyu, Chengqi, Zhang, Shilong, Wang, Yudong, Zheng, Miao, Zhao, Qian, Liu, Kuikun, Zhang, Wenwei, Luo, Ping, Chen, Kai.* arXiv preprint arXiv:2305.04790, 2023. [[abs](https://arxiv.org/abs/2305.04790)]
17. **Llama-adapter v2: Parameter-efficient visual instruction model** *Gao, Peng, Han, Jiaming, Zhang, Renrui, Lin, Ziyi, Geng, Shijie, Zhou, Aojun, Zhang, Wei, Lu, Pan, He, Conghui, Yue, Xiangyu, others.* arXiv preprint arXiv:2304.15010, 2023. [[abs](https://arxiv.org/abs/2304.15010)]
18. **Gemini: a family of highly capable multimodal models** *Team, Gemini, Anil, Rohan, Borgeaud, Sebastian, Wu, Yonghui, Alayrac, Jean-Baptiste, Yu, Jiahui, Soricut, Radu, Schalkwyk, Johan, Dai, Andrew M, Hauth, Anja, others.* arXiv preprint arXiv:2312.11805, 2023. [[abs](https://arxiv.org/abs/2312.11805)]
19. **Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic** *Chen, Keqin, Zhang, Zhao, Zeng, Weili, Zhang, Richong, Zhu, Feng, Zhao, Rui.* arXiv preprint arXiv:2306.15195, 2023. [[abs](https://arxiv.org/abs/2306.15195)]
20. **SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models** *Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, Jiaming Han, Siyuan Huang, Yichi Zhang, Xuming He, Hongsheng Li, Yu Jiao Qiao.* ArXiv, 2023.
21. **Gpt4tools: Teaching large language model to use tools via self-instruction** *Yang, Rui, Song, Lin, Li, Yanwei, Zhao, Sijie, Ge, Yixiao, Li, Xiu, Shan, Ying.* arXiv preprint arXiv:2305.18752, 2023. [[abs](https://arxiv.org/abs/2305.18752)]
22. **LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment** *Zhu, Bin, Lin, Bin, Ning, Munan, Yan, Yang, Cui, Jiaxi, Wang, HongFa, Pang, Yatian, Jiang, Wenhao, Zhang, Junwu, Li, Zongwei, others.* arXiv preprint arXiv:2310.01852, 2023. [[abs](https://arxiv.org/abs/2310.01852)]
23. **Imagebind: One embedding space to bind them all** *Girdhar, Rohit, El-Nouby, Alaaeldin, Liu, Zhuang, Singh, Mannat, Alwala, Kalyan Vasudev, Joulin, Armand, Misra, Ishan.* Presented at Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. [Link](No URL)
24. **Mobilevlm: A fast, reproducible and strong vision language assistant for mobile devices** *Chu, Xiangxiang, Qiao, Limeng, Lin, Xinyang, Xu, Shuang, Yang, Yang, Hu, Yiming, Wei, Fei, Zhang, Xinyu, Zhang, Bo, Wei, Xiaolin, others.* arXiv preprint arXiv:2312.16886, 2023. [[abs](https://arxiv.org/abs/2312.16886)]
25. **What Makes for Good Visual Tokenizers for Large Language Models?** *Wang, Guangzhi, Ge, Yixiao, Ding, Xiaohan, Kankanhalli, Mohan, Shan, Ying.* arXiv preprint arXiv:2305.12223, 2023. [[abs](https://arxiv.org/abs/2305.12223)]

### 2.2 Reasoning
1. **Chain-of-Thought Prompting Elicits Reasoning in Large Language Models**. *Jason Wei* et al. NeurIPS 2022. [[paper](https://arxiv.org/abs/2201.11903)]
2. **Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs**. *Maarten Sap* et al. EMNLP 2022. [[paper](https://arxiv.org/abs/2210.13312)]
3. **Inner Monologue: Embodied Reasoning through Planning with Language Models**. *Wenlong Huang* et al. CoRL 2022. [[paper](https://arxiv.org/abs/2207.05608)]
4. **Survey of Hallucination in Natural Language Generation**. *Ziwei Ji* et al. ACM Computing Surveys 2022. [[paper](https://arxiv.org/abs/2202.03629)]
5. **ReAct: Synergizing Reasoning and Acting in Language Models**. *Shunyu Yao* et al. ICLR 2023. [[paper](https://arxiv.org/abs/2210.03629)]
6. **Decomposed Prompting: A Modular Approach for Solving Complex Tasks**. *Tushar Khot* et al. ICLR 2023. [[paper](https://arxiv.org/abs/2210.02406)]
7. **Complexity-Based Prompting for Multi-Step Reasoning**. *Yao Fu* et al. ICLR 2023. [[paper](https://arxiv.org/abs/2210.00720)]
8. **Least-to-Most Prompting Enables Complex Reasoning in Large Language Models**. *Denny Zhou* et al. ICLR 2023. [[paper](https://arxiv.org/abs/2205.10625)]
9. **Towards Reasoning in Large Language Models: A Survey**. *Jie Huang* et al. ACL Findings 2023. [[paper](https://arxiv.org/abs/2212.10403)]
10. **ProgPrompt: Generating Situated Robot Task Plans using Large Language Models**. *Ishika Singh* et al. ICRA 2023. [[paper](https://arxiv.org/abs/2209.11302)]
11. **Reasoning with Language Model is Planning with World Model**. *Shibo Hao* et al. EMNLP 2023. [[paper](https://arxiv.org/abs/2305.14992)]
12. **Evaluating Object Hallucination in Large Vision-Language Models**. *Yifan Li* et al. EMNLP 2023. [[paper](https://arxiv.org/abs/2305.10355)]
13. **Tree of Thoughts: Deliberate Problem Solving with Large Language Models**. *Shunyu Yao* et al. NeurIPS 2023. [[paper](https://arxiv.org/abs/2305.10601)]
14. **Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents**. *Zihao Wang* et al. NeurIPS 2023. [[paper](https://arxiv.org/abs/2302.01560)]
15. **LLM+P: Empowering Large Language Models with Optimal Planning Proficiency**. *Bo Liu* et al. arXiv 2023. [[paper](https://arxiv.org/abs/2304.11477)]
16. **Language Models, Agent Models, and World Models: The LAW for Machine Reasoning and Planning**. *Zhiting Hu* et al. arXiv 2023. [[paper](https://arxiv.org/abs/2312.05230)]
17. **MMToM-QA: Multimodal Theory of Mind Question Answering**. *Chuanyang Jin* et al. arXiv 2024. [[paper](https://arxiv.org/abs/2401.08743)]
18. **Graph of Thoughts: Solving Elaborate Problems with Large Language Models**. *Maciej Besta* et al. AAAI 2024. [[paper](https://arxiv.org/abs/2308.09687)]
19. **Self-Refine: Iterative Refinement with Self-Feedback**. *Aman Madaan* et al. NeurIPS 2024. [[paper](https://arxiv.org/abs/2303.17651)]
20. **Reflexion: Language Agents with Verbal Reinforcement Learning**. *Noah Shinn* et al. NeurIPS 2024. [[paper](https://arxiv.org/abs/2303.11366)]

### 2.3 Memory
1. **Dense Passage Retrieval for Open-Domain Question Answering**. *Vladimir Karpukhin* et al. EMNLP 2020. [[paper](https://arxiv.org/abs/2004.04906)]
2. **Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks**. *Patrick Lewis* et al. NeurIPS 2020. [[paper](https://arxiv.org/abs/2005.11401)]
3. **REALM: Retrieval-Augmented Language Model Pre-Training**. *Kelvin Guu* et al. ICML 2020. [[paper](https://arxiv.org/abs/2002.08909)]
4. **Retrieval Augmentation Reduces Hallucination in Conversation**. *Kurt Shuster* et al. EMNLP Findings 2021. [[paper](https://arxiv.org/abs/2104.07567)]
5. **Improving Language Models by Retrieving from Trillions of Tokens**. *Sebastian Borgeaud* et al. ICML 2022. [[paper](https://arxiv.org/abs/2112.04426)]
6. **Generative Agents: Interactive Simulacra of Human Behavior**. *Joon Sung Park* et al. UIST 2023. [[paper](https://arxiv.org/abs/2304.03442)]
7. **Cognitive Architectures for Language Agents**. *Theodore R. Sumers* et al. TMLR 2024. [[paper](https://arxiv.org/abs/2309.02427)]

### 2.4 Metacognition
1. **Evolving Self-supervised Neural Networks: Autonomous Intelligence from Evolved Self-teaching**
   *Nam Le.* arXiv, 2019. [\[eprint\]](https://arxiv.org/abs/1906.08865)

2. **Self-Instruct: Aligning Language Models with Self-Generated Instructions**
   *Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, Hannaneh Hajishirzi.* arXiv, 2022. [[eprint](https://arxiv.org/abs/2212.10560)]

3. **ReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent**
   *Renat Aksitov, Sobhan Miryoosefi, Zonglin Li, Daliang Li, Sheila Babayan, Kavya Kopparapu, Zachary Fisher, Ruiqi Guo, Sushant Prakash, Pranesh Srinivasan, Manzil Zaheer, Felix Yu, Sanjiv Kumar.* arXiv, 2023. [\[eprint\]](https://arxiv.org/abs/2312.10003)

4. **Wizardlm: Empowering large language models to follow complex instructions**
   *Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Daxin Jiang.* arXiv, 2023. [[abs](https://arxiv.org/abs/2304.12244)]

## 3. AGI Interface: Connecting the World with AGI
### 3.1 Interfaces to Digital World
### 3.2 Interfaces to Physical World
### 3.3 Interfaces to Intelligence

#### 3.3.1 Interfaces to AI Agents

1. **Explanations from large language models make small reasoners better** *Li, Shiyang, Chen, Jianshu, Shen, Yelong, Chen, Zhiyu, Zhang, Xinlu, Li, Zekun, Wang, Hong, Qian, Jing, Peng, Baolin, Mao, Yi, others.* arXiv preprint arXiv:2210.06726, 2022. [[abs](https://arxiv.org/abs/2210.06726)]
2. **Tailoring self-rationalizers with multi-reward distillation** *Ramnath, Sahana, Joshi, Brihi, Hallinan, Skyler, Lu, Ximing, Li, Liunian Harold, Chan, Aaron, Hessel, Jack, Choi, Yejin, Ren, Xiang.* arXiv preprint arXiv:2311.02805, 2023. [[abs](https://arxiv.org/abs/2311.02805)]
3. **OpenMoE: Open Mixture-of-Experts Language Models** *Fuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zangwei Zheng, Wangchunshu Zhou, Yang You.* arXiv 2023. [[abs](https://arxiv.org/abs/No eprint ID)]
4. **Knowledge Distillation of Large Language Models** *Yuxian Gu, Li Dong, Furu Wei, Minlie Huang.* arXiv 2023. [[abs](https://arxiv.org/abs/2306.08543)]
5. **Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes** *Hsieh, Cheng-Yu, Li, Chun-Liang, Yeh, Chih-Kuan, Nakhost, Hootan, Fujii, Yasuhisa, Ratner, Alexander, Krishna, Ranjay, Lee, Chen-Yu, Pfister, Tomas.* arXiv preprint arXiv:2305.02301, 2023. [[abs](https://arxiv.org/abs/2305.02301)]
6. **Phi-2: The surprising power of small language models** *Javaheripi, Mojan, Bubeck, S\'e.* Microsoft Research Blog, 2023.
7. **The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning** *Yuchen Lin, Bill, Ravichander, Abhilasha, Lu, Ximing, Dziri, Nouha, Sclar, Melanie, Chandu, Khyathi, Bhagavatula, Chandra, Choi, Yejin.* arXiv preprint arXiv:arXiv e-prints, 2023. [[abs](https://arxiv.org/abs/arXiv e-prints)]
8. **LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents** *Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun Zhu, Lei Zhang, Jianfeng Gao, Chunyuan Li.* arXiv preprint arXiv:2311.05437, 2023. [[abs](https://arxiv.org/abs/2311.05437)]
9. **Neural amortized inference for nested multi-agent reasoning** *Jha, Kunal, Le, Tuan Anh, Jin, Chuanyang, Kuo, Yen-Ling, Tenenbaum, Joshua B, Shu, Tianmin.* arXiv preprint arXiv:2308.11071, 2023. [[abs](https://arxiv.org/abs/2308.11071)]
10. **Textbooks are all you need ii: phi-1.5 technical report** *Li, Yuanzhi, Bubeck, S\'e.* arXiv preprint arXiv:2309.05463, 2023. [[abs](https://arxiv.org/abs/2309.05463)]
11. **Lion: Adversarial Distillation of Closed-Source Large Language Model** *Jiang, Yuxin, Chan, Chunkit, Chen, Mingyang, Wang, Wei.* arXiv preprint arXiv:2305.12870, 2023. [[abs](https://arxiv.org/abs/2305.12870)]
12. **Patch-level Routing in Mixture-of-Experts is Provably Sample-efficient for Convolutional Neural Networks** *Chowdhury, Mohammed Nowaz Rabbani, Zhang, Shuai, Wang, Meng, Liu, Sijia, Chen, Pin-Yu.* arXiv preprint arXiv:2306.04073, 2023. [[abs](https://arxiv.org/abs/2306.04073)]
13. **Symbolic chain-of-thought distillation: Small models can also" think" step-by-step** *Li, Liunian Harold, Hessel, Jack, Yu, Youngjae, Ren, Xiang, Chang, Kai-Wei, Choi, Yejin.* arXiv preprint arXiv:2306.14050, 2023. [[abs](https://arxiv.org/abs/2306.14050)]
14. **ChatGPT outperforms crowd workers for text-annotation tasks** *Gilardi, Fabrizio, Alizadeh, Meysam, Kubli, Ma\"e.* Proceedings of the National Academy of Sciences, 2023. [[abs](https://arxiv.org/abs/2303.15056)]
15. **Fairness, Accountability, Transparency, and Ethics (FATE) in Artificial Intelligence (AI), and higher education: A systematic review** *Memarian, Bahar, Doleck, Tenzin.* Computers and Education: Artificial Intelligence, 2023. [Link](https://www.sciencedirect.com/science/article/pii/S2666920X23000310)
16. **Metagpt: Meta programming for multi-agent collaborative framework** *Hong, Sirui, Zheng, Xiawu, Chen, Jonathan, Cheng, Yuheng, Wang, Jinlin, Zhang, Ceyao, Wang, Zili, Yau, Steven Ka Shing, Lin, Zijuan, Zhou, Liyang, others.* arXiv preprint arXiv:2308.00352, 2023. [[abs](https://arxiv.org/abs/2308.00352)]
17. **Autoagents: A framework for automatic agent generation** *Chen, Guangyao, Dong, Siwei, Shu, Yu, Zhang, Ge, Sesay, Jaward, Karlsson, B\"o.* arXiv preprint arXiv:2309.17288, 2023. [[abs](https://arxiv.org/abs/2309.17288)]
18. **Specializing Smaller Language Models towards Multi-Step Reasoning** *Fu, Yao, Peng, Hao, Ou, Litu, Sabharwal, Ashish, Khot, Tushar.* arXiv preprint arXiv:2301.12726, 2023. [[abs](https://arxiv.org/abs/2301.12726)]
19. **Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision** *Burns, Collin, Izmailov, Pavel, Kirchner, Jan Hendrik, Baker, Bowen, Gao, Leo, Aschenbrenner, Leopold, Chen, Yining, Ecoffet, Adrien, Joglekar, Manas, Leike, Jan, others.* arXiv preprint arXiv:2312.09390, 2023. [[abs](https://arxiv.org/abs/2312.09390)]
20. **Mindstorms in natural language-based societies of mind** *Zhuge, Mingchen, Liu, Haozhe, Faccio, Francesco, Ashley, Dylan R, Csord\'a.* arXiv preprint arXiv:2305.17066, 2023. [[abs](https://arxiv.org/abs/2305.17066)]
21. **Dynamic llm-agent network: An llm-agent collaboration framework with agent team optimization** *Liu, Zijun, Zhang, Yanzhe, Li, Peng, Liu, Yang, Yang, Diyi.* arXiv preprint arXiv:2310.02170, 2023. [[abs](https://arxiv.org/abs/2310.02170)]
22. **Agents: An open-source framework for autonomous language agents** *Zhou, Wangchunshu, Jiang, Yuchen Eleanor, Li, Long, Wu, Jialong, Wang, Tiannan, Qiu, Shi, Zhang, Jintian, Chen, Jing, Wu, Ruipu, Wang, Shuai, others.* arXiv preprint arXiv:2309.07870, 2023. [[abs](https://arxiv.org/abs/2309.07870)]
23. **Stanford Alpaca: An Instruction-following LLaMA model** *Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B. Hashimoto.* arXiv 2023. [[abs](https://arxiv.org/abs/No eprint ID)]
24. **Octavius: Mitigating Task Interference in MLLMs via MoE** *Chen, Zeren, Wang, Ziqin, Wang, Zhen, Liu, Huayang, Yin, Zhenfei, Liu, Si, Sheng, Lu, Ouyang, Wanli, Qiao, Yu, Shao, Jing.* arXiv preprint arXiv:2311.02684, 2023. [[abs](https://arxiv.org/abs/2311.02684)]
25. **Enhancing chat language models by scaling high-quality instructional conversations** *Ding, Ning, Chen, Yulin, Xu, Bokai, Qin, Yujia, Zheng, Zhi, Hu, Shengding, Liu, Zhiyuan, Sun, Maosong, Zhou, Bowen.* arXiv preprint arXiv:2305.14233, 2023. [[abs](https://arxiv.org/abs/2305.14233)]
26. **Communicative agents for software development** *Qian, Chen, Cong, Xin, Yang, Cheng, Chen, Weize, Su, Yusheng, Xu, Juyuan, Liu, Zhiyuan, Sun, Maosong.* arXiv preprint arXiv:2307.07924, 2023. [[abs](https://arxiv.org/abs/2307.07924)]
27. **Principle-driven self-alignment of language models from scratch with minimal human supervision** *Sun, Zhiqing, Shen, Yikang, Zhou, Qinhong, Zhang, Hongxin, Chen, Zhenfang, Cox, David, Yang, Yiming, Gan, Chuang.* Advances in Neural Information Processing Systems, 2024. [[abs](https://arxiv.org/abs/2305.03047)]
28. **Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision** *Sun, Zhiqing, Yu, Longhui, Shen, Yikang, Liu, Weiyang, Yang, Yiming, Welleck, Sean, Gan, Chuang.* arXiv preprint arXiv:2403.09472, 2024. [[abs](https://arxiv.org/abs/2403.09472)]

#### 3.3.2 Interfaces to Humans

1. **Guidelines for Human-AI Interaction**
   *Saleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira Nushi, Penny Collisson, Jina Suh, Shamsi Iqbal, Paul N. Bennett, Kori Inkpen, Jaime Teevan, Ruth Kikin-Gil, Eric Horvitz*. CHI 2019. [[paper](https://dl.acm.org/doi/10.1145/3290605.3300233)]
2. **Design Principles for Generative AI Applications**
   *Justin D. Weisz, Jessica He, Michael Muller, Gabriela Hoefer, Rachel Miles, Werner Geyer*. CHI 2024. [[paper](http://arxiv.org/abs/2401.14484)]
3. **Graphologue: Exploring Large Language Model Responses with Interactive Diagrams**
   *Peiling Jiang, Jude Rayan, Steven P. Dow, Haijun Xia*. UIST 2023. [[paper](https://dl.acm.org/doi/10.1145/3586183.3606737)]
4. **Sensecape: Enabling Multilevel Exploration and Sensemaking with Large Language Models**
   *Sangho Suh, Bryan Min, Srishti Palani, Haijun Xia*. UIST 2023. [[paper](https://dl.acm.org/doi/10.1145/3586183.3606756)]
5. **Supporting Sensemaking of Large Language Model Outputs at Scale**
   *Katy Ilonka Gero, Chelse Swoopes, Ziwei Gu, Jonathan K. Kummerfeld, Elena L. Glassman*. CHI 2024. [[paper](https://arxiv.org/abs/2401.13726)]
6. **Luminate: Structured Generation and Exploration of Design Space with Large Language Models for Human-AI Co-Creation**
   *Sangho Suh, Meng Chen, Bryan Min, Toby Jia-Jun Li, Haijun Xia*. CHI 2024. [[Paper](http://arxiv.org/abs/2310.12953)]
7. **AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts**
   *Tongshuang Wu, Michael Terry, Carrie Jun Cai*. CHI 2022. [[paper](https://dl.acm.org/doi/10.1145/3491102.3517582)]
8. **Promptify: Text-to-Image Generation through Interactive Prompt Exploration with Large Language Models**
   *Stephen Brade, Bryan Wang, Mauricio Sousa, Sageev Oore, Tovi Grossman*. CHI 2023. [[paper](https://dl.acm.org/doi/10.1145/3586183.3606725)]
9. **ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing**
   *Ian Arawjo, Chelse Swoopes, Priyan Vaithilingam, Martin Wattenberg, Elena Glassman*. CHI 2024. [[paper](https://doi.org/10.48550/arXiv.2309.09128)]
10. **CoPrompt: Supporting Prompt Sharing and Referring in Collaborative Natural Language Programming**
    *Li Feng, Ryan Yen, Yuzhe You, Mingming Fan, Jian Zhao, Zhicong Lu*. CHI 2024. [[paper](http://arxiv.org/abs/2310.09235)]
11. **Generating Automatic Feedback on UI Mockups with Large Language Models**
    *Peitong Duan, Jeremy Warner, Yang Li, Bj√∂rn Hartmann*. CHI 2024. [[paper](http://arxiv.org/abs/2403.13139)]
12. **Rambler: Supporting Writing With Speech via LLM-Assisted Gist Manipulation**
    *Susan Lin, Jeremy Warner, J. D. Zamfirescu-Pereira, Matthew G. Lee, Sauhard Jain, Michael Xuelin Huang, Piyawat Lertvittayakumjorn, Shanqing Cai, Shumin Zhai, Bj√∂rn Hartmann, Can Liu*. CHI 2024. [[paper](http://arxiv.org/abs/2401.10838)]
13. **Embedding Large Language Models into Extended Reality: Opportunities and Challenges for Inclusion, Engagement, and Privacy**
    *Efe Bozkir, S√ºleyman √ñzdel, Ka Hei Carrie Lau, Mengdi Wang, Hong Gao, Enkelejda Kasneci*. arXiv 2024. [[paper](http://arxiv.org/abs/2402.03907)]
14. **GenAssist: Making Image Generation Accessible**
    *Mina Huh, Yi-Hao Peng, Amy Pavel*. UIST 2023. [[paper](https://dl.acm.org/doi/10.1145/3586183.3606735)]
15. **‚ÄúThe less I type, the better‚Äù: How AI Language Models can Enhance or Impede Communication for AAC Users**
    *Stephanie Valencia, Richard Cave, Krystal Kallarackal, Katie Seaver, Michael Terry, Shaun K. Kane*. CHI 2023. [[paper](https://dl.acm.org/doi/10.1145/3544548.3581560)]
16. **Re-examining Whether, Why, and How Human-AI Interaction Is Uniquely Difficult to Design**
    *Qian Yang, Aaron Steinfeld, Carolyn Ros√©, John Zimmerman*. CHI 2020. [[paper](https://dl.acm.org/doi/10.1145/3313831.3376301)]

## 4. AGI Systems: Implementing the Mechanism of AGI

### 4.2 Scalable Model Architectures
1. **Outrageously large neural networks: The sparsely-gated mixture-of-experts layer** *Shazeer, Noam, Mirhoseini, Azalia, Maziarz, Krzysztof, Davis, Andy, Le, Quoc, Hinton, Geoffrey, Dean, Jeff.* arXiv preprint arXiv:1701.06538, 2017. [[abs](https://arxiv.org/abs/1701.06538)]
2. **Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention** *Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, Fran√ßois Fleuret.* arXiv 2020. [[abs](https://arxiv.org/abs/2006.16236)]
3. **Longformer: The Long-Document Transformer** *Iz Beltagy, Matthew E. Peters, Arman Cohan.* arXiv 2020. [[abs](https://arxiv.org/abs/2004.05150)]
4. **LightSeq: A High Performance Inference Library for Transformers** *Xiaohui Wang, Ying Xiong, Yang Wei, Mingxuan Wang, Lei Li.* arXiv 2021. [[abs](https://arxiv.org/abs/2010.13887)]
5. **Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity** *William Fedus, Barret Zoph, Noam Shazeer.* arXiv 2022. [[abs](https://arxiv.org/abs/2101.03961)]
6. **Efficiently Modeling Long Sequences with Structured State Spaces** *Albert Gu, Karan Goel, Christopher R√©.* arXiv 2022. [[abs](https://arxiv.org/abs/2111.00396)]
7. **MegaBlocks: Efficient Sparse Training with Mixture-of-Experts** *Trevor Gale, Deepak Narayanan, Cliff Young, Matei Zaharia.* arXiv 2022. [[abs](https://arxiv.org/abs/2211.15841)]
8. **Training Compute-Optimal Large Language Models** *Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, Laurent Sifre.* arXiv 2022. [[abs](https://arxiv.org/abs/2203.15556)]
9. **Effective Long-Context Scaling of Foundation Models** *Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, Hao Ma.* arXiv 2023. [[abs](https://arxiv.org/abs/2309.16039)]
10. **Hyena Hierarchy: Towards Larger Convolutional Language Models** *Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y. Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, Christopher R√©.* arXiv 2023. [[abs](https://arxiv.org/abs/2302.10866)]
11. **Stanford Alpaca: An Instruction-following LLaMA model** *Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B. Hashimoto.* arXiv 2023. [[abs](https://arxiv.org/abs/No eprint ID)]
12. **Rwkv: Reinventing rnns for the transformer era** *Peng, Bo, Alcaide, Eric, Anthony, Quentin, Albalak, Alon, Arcadinho, Samuel, Cao, Huanqi, Cheng, Xin, Chung, Michael, Grella, Matteo, GV, Kranthi Kiran, others.* arXiv preprint arXiv:2305.13048, 2023. [[abs](https://arxiv.org/abs/2305.13048)]
13. **Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time** *Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, Beidi Chen.* arXiv 2023. [[abs](https://arxiv.org/abs/2310.17157)]
14. **Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity** *Haojun Xia, Zhen Zheng, Yuchao Li, Donglin Zhuang, Zhongzhu Zhou, Xiafei Qiu, Yong Li, Wei Lin, Shuaiwen Leon Song.* arXiv 2023. [[abs](https://arxiv.org/abs/2309.10285)]
15. **ByteTransformer: A High-Performance Transformer Boosted for Variable-Length Inputs** *Yujia Zhai, Chengquan Jiang, Leyuan Wang, Xiaoying Jia, Shang Zhang, Zizhong Chen, Xin Liu, Yibo Zhu.* arXiv 2023. [[abs](https://arxiv.org/abs/2210.03052)]
16. **Tutel: Adaptive Mixture-of-Experts at Scale** *Changho Hwang, Wei Cui, Yifan Xiong, Ziyue Yang, Ze Liu, Han Hu, Zilong Wang, Rafael Salas, Jithin Jose, Prabhat Ram, Joe Chau, Peng Cheng, Fan Yang, Mao Yang, Yongqiang Xiong.* arXiv 2023. [[abs](https://arxiv.org/abs/2206.03382)]
17. **Mamba: Linear-Time Sequence Modeling with Selective State Spaces** *Albert Gu, Tri Dao.* arXiv 2023. [[abs](https://arxiv.org/abs/2312.00752)]
18. **Hungry Hungry Hippos: Towards Language Modeling with State Space Models** *Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, Christopher R√©.* arXiv 2023. [[abs](https://arxiv.org/abs/2212.14052)]
19. **Retentive Network: A Successor to Transformer for Large Language Models** *Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, Furu Wei.* ArXiv, 2023.
20. **Mechanistic Design and Scaling of Hybrid Architectures** *Michael Poli, Armin W Thomas, Eric Nguyen, Pragaash Ponnusamy, Bj√∂rn Deiseroth, Kristian Kersting, Taiji Suzuki, Brian Hie, Stefano Ermon, Christopher R√©, Ce Zhang, Stefano Massaroli.* arXiv 2024. [[abs](https://arxiv.org/abs/2403.17844)]

### 4.3 Large-scale Training
1. **Training Deep Nets with Sublinear Memory Cost** *Tianqi Chen, Bing Xu, Chiyuan Zhang, Carlos Guestrin.* arXiv 2016. [[abs](https://arxiv.org/abs/1604.06174)]
2. **Beyond Data and Model Parallelism for Deep Neural Networks** *Zhihao Jia, Matei Zaharia, Alex Aiken.* arXiv 2018. [[abs](https://arxiv.org/abs/1807.05358)]
3. **GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism** *Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, Zhifeng Chen.* arXiv 2019. [[abs](https://arxiv.org/abs/1811.06965)]
4. **Parameter-efficient transfer learning for NLP** *Houlsby, Neil, Giurgiu, Andrei, Jastrzebski, Stanislaw, Morrone, Bruna, De Laroussilhe, Quentin, Gesmundo, Andrea, Attariyan, Mona, Gelly, Sylvain.* Presented at International conference on machine learning, 2019. [Link](No URL)
5. **Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism** *Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, Bryan Catanzaro.* arXiv 2020. [[abs](https://arxiv.org/abs/1909.08053)]
6. **Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning** *Lianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng Chen, Yanping Huang, Yida Wang, Yuanzhong Xu, Danyang Zhuo, Eric P. Xing, Joseph E. Gonzalez, Ion Stoica.* arXiv 2022. [[abs](https://arxiv.org/abs/2201.12023)]
7. **DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale** *Reza Yazdani Aminabadi, Samyam Rajbhandari, Minjia Zhang, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Jeff Rasley, Shaden Smith, Olatunji Ruwase, Yuxiong He.* arXiv 2022. [[abs](https://arxiv.org/abs/2207.00032)]
8. **Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models** *Kushal Tirumala, Aram H. Markosyan, Luke Zettlemoyer, Armen Aghajanyan.* arXiv 2022. [[abs](https://arxiv.org/abs/2205.10770)]
9. **SWARM Parallelism: Training Large Models Can Be Surprisingly Communication-Efficient** *Max Ryabinin, Tim Dettmers, Michael Diskin, Alexander Borzunov.* arXiv 2023. [[abs](https://arxiv.org/abs/2301.11913)]
10. **Training Trajectories of Language Models Across Scales** *Mengzhou Xia, Mikel Artetxe, Chunting Zhou, Xi Victoria Lin, Ramakanth Pasunuru, Danqi Chen, Luke Zettlemoyer, Ves Stoyanov.* arXiv 2023. [[abs](https://arxiv.org/abs/2212.09803)]
11. **HexGen: Generative Inference of Foundation Model over Heterogeneous Decentralized Environment** *Youhe Jiang, Ran Yan, Xiaozhe Yao, Beidi Chen, Binhang Yuan.* arXiv 2023. [[abs](https://arxiv.org/abs/2311.11514)]
12. **FusionAI: Decentralized Training and Deploying LLMs with Massive Consumer-Level GPUs** *Zhenheng Tang, Yuxin Wang, Xin He, Longteng Zhang, Xinglin Pan, Qiang Wang, Rongfei Zeng, Kaiyong Zhao, Shaohuai Shi, Bingsheng He, Xiaowen Chu.* arXiv 2023. [[abs](https://arxiv.org/abs/2309.01172)]
13. **Ring Attention with Blockwise Transformers for Near-Infinite Context** *Hao Liu, Matei Zaharia, Pieter Abbeel.* arXiv 2023. [[abs](https://arxiv.org/abs/2310.01889)]
14. **Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling** *Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, Oskar van der Wal.* arXiv 2023. [[abs](https://arxiv.org/abs/2304.01373)]
15. **Fine-tuning Language Models over Slow Networks using Activation Compression with Guarantees** *Jue Wang, Binhang Yuan, Luka Rimanic, Yongjun He, Tri Dao, Beidi Chen, Christopher Re, Ce Zhang.* arXiv 2023. [[abs](https://arxiv.org/abs/2206.01299)]
16. **LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention** *Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Yu Qiao.* arXiv 2023. [[abs](https://arxiv.org/abs/2303.16199)]
17. **QLoRA: Efficient Finetuning of Quantized LLMs** *Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer.* arXiv 2023. [[abs](https://arxiv.org/abs/2305.14314)]
18. **Efficient Memory Management for Large Language Model Serving with PagedAttention** *Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, Ion Stoica.* arXiv 2023. [[abs](https://arxiv.org/abs/2309.06180)]
19. **Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache** *Bin Lin, Tao Peng, Chen Zhang, Minmin Sun, Lanbo Li, Hanyu Zhao, Wencong Xiao, Qi Xu, Xiafei Qiu, Shen Li, Zhigang Ji, Yong Li, Wei Lin.* arXiv 2024. [[abs](https://arxiv.org/abs/2401.02669)]
20. **OLMo: Accelerating the Science of Language Models** *Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, Hannaneh Hajishirzi.* arXiv 2024. [[abs](https://arxiv.org/abs/2402.00838)]

### 4.4 Inference Techniques
1. **FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness**. *Tri Dao* et al. NeurIPS 2022. [[paper](https://arxiv.org/abs/2205.14135)]
2. **Draft \& Verify: Lossless Large Language Model Acceleration via Self-Speculative Decoding** *Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen, Gang Chen, Sharad Mehrotra.* arXiv 2023. [[abs](https://arxiv.org/abs/2309.08168)]
3. **Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems** *Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Hongyi Jin, Tianqi Chen, Zhihao Jia.* arXiv 2023. [[abs](https://arxiv.org/abs/2312.15234)]
4. **FlashDecoding++: Faster Large Language Model Inference on GPUs** *Ke Hong, Guohao Dai, Jiaming Xu, Qiuli Mao, Xiuhong Li, Jun Liu, Kangdi Chen, Yuhan Dong, Yu Wang.* arXiv 2023. [[abs](https://arxiv.org/abs/2311.01282)]
5. **Fast Inference from Transformers via Speculative Decoding** *Yaniv Leviathan, Matan Kalman, Yossi Matias.* arXiv 2023. [[abs](https://arxiv.org/abs/2211.17192)]
6. **Efficient Memory Management for Large Language Model Serving with PagedAttention** *Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, Ion Stoica.* arXiv 2023. [[abs](https://arxiv.org/abs/2309.06180)]
7. **Fast Distributed Inference Serving for Large Language Models** *Bingyang Wu, Yinmin Zhong, Zili Zhang, Gang Huang, Xuanzhe Liu, Xin Jin.* arXiv 2023. [[abs](https://arxiv.org/abs/2305.05920)]
8. **S-LoRA: Serving Thousands of Concurrent LoRA Adapters** *Ying Sheng, Shiyi Cao, Dacheng Li, Coleman Hooper, Nicholas Lee, Shuo Yang, Christopher Chou, Banghua Zhu, Lianmin Zheng, Kurt Keutzer, Joseph E. Gonzalez, Ion Stoica.* arXiv 2023. [[abs](https://arxiv.org/abs/2311.03285)]
9. **TensorRT-LLM: A TensorRT Toolbox for Optimized Large Language Model Inference** *NVIDIA.* arXiv 2023. [[abs](https://arxiv.org/abs/No eprint ID)]
10. **Punica: Multi-Tenant LoRA Serving** *Lequn Chen, Zihao Ye, Yongji Wu, Danyang Zhuo, Luis Ceze, Arvind Krishnamurthy.* arXiv 2023. [[abs](https://arxiv.org/abs/2310.18547)]
11. **S$^3** *Yunho Jin, Chun-Feng Wu, David Brooks, Gu-Yeon Wei.* arXiv 2023. [[abs](https://arxiv.org/abs/2306.06000)]
12. **Multi-LoRA inference server that scales to 1000s of fine-tuned LLMs** *Predibase.* arXiv 2023. [[abs](https://arxiv.org/abs/No eprint ID)]
13. **Prompt Lookup Decoding** *Apoorv Saxena.* arXiv 2023. [[abs](https://arxiv.org/abs/No eprint ID)]
14. **FasterTransformer** *NVIDIA.* arXiv 2023. [[abs](https://arxiv.org/abs/No eprint ID)]
15. **DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference** *Connor Holmes, Masahiro Tanaka, Michael Wyatt, Ammar Ahmad Awan, Jeff Rasley, Samyam Rajbhandari, Reza Yazdani Aminabadi, Heyang Qin, Arash Bakhtiari, Lev Kurilenko, Yuxiong He.* arXiv 2024. [[abs](https://arxiv.org/abs/2401.08671)]
16. **SpecInfer: Accelerating Generative Large Language Model Serving with Tree-based Speculative Inference and Verification** *Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Zhengxin Zhang, Rae Ying Yee Wong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, Chunan Shi, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, Zhihao Jia.* arXiv 2024. [[abs](https://arxiv.org/abs/2305.09781)]
17. **Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads** *Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D. Lee, Deming Chen, Tri Dao.* arXiv 2024. [[abs](https://arxiv.org/abs/2401.10774)]


### 4.5 Cost and Efficiency

1. **Demonstrate-Search-Predict: Composing Retrieval and Language Models for Knowledge-Intensive NLP** *No author.* No journal, No year.
2. **Automated Machine Learning: Methods, Systems, Challenges** *Hutter, Frank, Kotthoff, Lars, Vanschoren, Joaquin.* Springer Publishing Company, Incorporated, 2019.
3. **Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time** *Mitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, Ludwig Schmidt.* arXiv 2022. [[abs](https://arxiv.org/abs/2203.05482)]
4. **Data Debugging with Shapley Importance over End-to-End Machine Learning Pipelines** *Bojan Karla≈°, David Dao, Matteo Interlandi, Bo Li, Sebastian Schelter, Wentao Wu, Ce Zhang.* arXiv 2022. [[abs](https://arxiv.org/abs/2204.11131)]
5. **Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference** *Chi Wang, Susan Xueqing Liu, Ahmed H. Awadallah.* arXiv 2023. [[abs](https://arxiv.org/abs/2303.04673)]
6. **Large Language Models Are Human-Level Prompt Engineers** *Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, Jimmy Ba.* arXiv 2023. [[abs](https://arxiv.org/abs/2211.01910)]
7. **Merging by Matching Models in Task Subspaces** *Derek Tam, Mohit Bansal, Colin Raffel.* arXiv 2023. [[abs](https://arxiv.org/abs/2312.04339)]
8. **Editing Models with Task Arithmetic** *Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, Ali Farhadi.* arXiv 2023. [[abs](https://arxiv.org/abs/2212.04089)]
9. **PriorBand: Practical Hyperparameter Optimization in the Age of Deep Learning** *Neeratyoy Mallik, Edward Bergman, Carl Hvarfner, Danny Stoll, Maciej Janowski, Marius Lindauer, Luigi Nardi, Frank Hutter.* arXiv 2023. [[abs](https://arxiv.org/abs/2306.12370)]
10. **An Empirical Study of Multimodal Model Merging** *Yi-Lin Sung, Linjie Li, Kevin Lin, Zhe Gan, Mohit Bansal, Lijuan Wang.* arXiv 2023. [[abs](https://arxiv.org/abs/2304.14933)]
11. **DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines** *Khattab, Omar, Singhvi, Arnav, Maheshwari, Paridhi, Zhang, Zhiyuan, Santhanam, Keshav, Vardhamanan, Sri, Haq, Saiful, Sharma, Ashutosh, Joshi, Thomas T., Moazam, Hanna, Miller, Heather, Zaharia, Matei, Potts, Christopher.* arXiv preprint arXiv:2310.03714, 2023. [[abs](https://arxiv.org/abs/2310.03714)]
12. **FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance** *Lingjiao Chen, Matei Zaharia, James Zou.* arXiv 2023. [[abs](https://arxiv.org/abs/2305.05176)]
13. **Tandem Transformers for Inference Efficient LLMs** *Aishwarya P S, Pranav Ajit Nair, Yashas Samaga, Toby Boyd, Sanjiv Kumar, Prateek Jain, Praneeth Netrapalli.* arXiv 2024. [[abs](https://arxiv.org/abs/2402.08644)]
14. **AIOS: LLM Agent Operating System** *Kai Mei, Zelong Li, Shuyuan Xu, Ruosong Ye, Yingqiang Ge, Yongfeng Zhang.* arXiv 2024. [[abs](https://arxiv.org/abs/2403.16971)]
15. **LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition** *Chengsong Huang, Qian Liu, Bill Yuchen Lin, Tianyu Pang, Chao Du, Min Lin.* arXiv 2024. [[abs](https://arxiv.org/abs/2307.13269)]
16. **AutoML in the Age of Large Language Models: Current Challenges, Future Opportunities and Risks** *Alexander Tornede, Difan Deng, Theresa Eimer, Joseph Giovanelli, Aditya Mohan, Tim Ruhkopf, Sarah Segel, Daphne Theodorakopoulos, Tanja Tornede, Henning Wachsmuth, Marius Lindauer.* arXiv 2024. [[abs](https://arxiv.org/abs/2306.08107)]

### 4.6 Computing Platforms
1. **TVM: An Automated End-to-End Optimizing Compiler for Deep Learning** *Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Meghan Cowan, Haichen Shen, Leyuan Wang, Yuwei Hu, Luis Ceze, Carlos Guestrin, Arvind Krishnamurthy.* arXiv 2018. [[abs](https://arxiv.org/abs/1802.04799)]
2. **TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings** *Norman P. Jouppi, George Kurian, Sheng Li, Peter Ma, Rahul Nagarajan, Lifeng Nai, Nishant Patil, Suvinay Subramanian, Andy Swing, Brian Towles, Cliff Young, Xiang Zhou, Zongwei Zhou, David Patterson.* arXiv 2023. [[abs](https://arxiv.org/abs/2304.01433)]


## 5. AGI Alignment: Reconciling Needs with AGI
### 5.1 Expectations of AGI Alignment

1. **Human Compatible: Artificial Intelligence and the Problem of Control**
   *Stuart Russell*. Viking, 2019.
2. **Artificial Intelligence, Values and Alignment**
   *Iason Gabriel*. Minds and Machines, 2020. [[paper](http://arxiv.org/abs/2001.09768)]
3. **Alignment of Language Agents**
   *Zachary Kenton, Tom Everitt, Laura Weidinger, Iason Gabriel, Vladimir Mikulik, Geoffrey Irving*. arXiv, 2021. [[Paper](http://arxiv.org/abs/2103.14659)]
4. **The Value Learning Problem** 
   *Nate Soares*. Machine Intelligence Research Institute Technical Report [[paper](https://intelligence.org/files/ValueLearningProblem.pdf)]
5. **Concrete Problems in AI Safety**
   *Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, Dan Man√©*. arXiv, 2016. [[paper](http://arxiv.org/abs/1606.06565)]
6. **Ethical and social risks of harm from Language Models**
   *Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, Zac Kenton, Sasha Brown, Will Hawkins, Tom Stepleton, Courtney Biles, Abeba Birhane, Julia Haas, Laura Rimell, Lisa Anne Hendricks, William Isaac, Sean Legassick, Geoffrey Irving, Iason Gabriel*. arXiv, 2021. [[paper](http://arxiv.org/abs/2112.04359)]
7. **On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?**
   *Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, Shmargaret Shmitchell*. FAccT 2021. [[paper](https://dl.acm.org/doi/10.1145/3442188.3445922)]
8. **The global landscape of AI ethics guidelines**
   *Anna Jobin, Marcello Ienca, Effy Vayena*. Nature Machine Intelligence, 2019. [[paper](https://www.nature.com/articles/s42256-019-0088-2)]
9. **Persistent Anti-Muslim Bias in Large Language Models**
   *Abubakar Abid, Maheen Farooqi, James Zou*. AIES, 2021. [[paper](http://arxiv.org/abs/2101.05783)]
10. **Toward Gender-Inclusive Coreference Resolution**
      *Yang Trista Cao, Hal Daum√© III*. ACL, 2020. [[paper](https://aclanthology.org/2020.acl-main.418)]
11. **The Social Impact of Natural Language Processing**
    *Dirk Hovy, Shannon L. Spruit*. ACL 2016. [[paper](https://aclanthology.org/P16-2096)]
12. **TruthfulQA: Measuring How Models Mimic Human Falsehoods**
    *Stephanie Lin, Jacob Hilton, Owain Evans*. ACL 2022. [[paper](https://aclanthology.org/2022.acl-long.229)]
13. **The Radicalization Risks of GPT-3 and Advanced Neural Language Models**
    *Kris McGuffie, Alex Newhouse*. arXiv, 2020. [[paper](http://arxiv.org/abs/2009.06807)]
14. **AI Transparency in the Age of LLMs: A Human-Centered Research Roadmap**
    *Q. Vera Liao, Jennifer Wortman Vaughan*. arXiv 2023. [[paper](http://arxiv.org/abs/2306.01941)]
15. **Beyond Expertise and Roles: A Framework to Characterize the Stakeholders of Interpretable Machine Learning and their Needs**
    *Harini Suresh, Steven R. Gomez, Kevin K. Nam, Arvind Satyanarayan*. CHI 2021. [[paper](https://dl.acm.org/doi/10.1145/3411764.3445088)]
16. **Identifying and Mitigating the Security Risks of Generative AI**
    *Clark Barrett, Brad Boyd, Elie Burzstein, Nicholas Carlini, Brad Chen, Jihye Choi, Amrita Roy Chowdhury, Mihai Christodorescu, Anupam Datta, Soheil Feizi, Kathleen Fisher, Tatsunori Hashimoto, Dan Hendrycks, Somesh Jha, Daniel Kang, Florian Kerschbaum, Eric Mitchell, John Mitchell, Zulfikar Ramzan, Khawaja Shams, Dawn Song, Ankur Taly, Diyi Yang*. arXiv, 2023. [[paper](http://arxiv.org/abs/2308.14840)]
17. **LLM Agents can Autonomously Hack Websites**
    *Richard Fang, Rohan Bindu, Akul Gupta, Qiusi Zhan, Daniel Kang*. arXiv, 2024. [[paper](http://arxiv.org/abs/2402.06664)]
18. **Deepfakes, Phrenology, Surveillance, and More! A Taxonomy of AI Privacy Risks**
    *Hao-Ping Lee, Yu-Ju Yang, Thomas Serban von Davier, Jodi Forlizzi, Sauvik Das*. CHI 2024. [[paper](http://arxiv.org/abs/2310.07879)]
19. **Privacy in the Age of AI**
    *Sauvik Das, Hao-Ping (Hank) Lee, Jodi Forlizzi*. Communications of the ACM, 2023. [[paper](https://dl.acm.org/doi/10.1145/3625254)]

### 5.2 Current Alignment Techniques

1. **Learning to summarize with human feedback** *Stiennon, Nisan, Ouyang, Long, Wu, Jeffrey, Ziegler, Daniel, Lowe, Ryan, Voss, Chelsea, Radford, Alec, Amodei, Dario, Christiano, Paul F.* Advances in Neural Information Processing Systems, 2020. [[abs](https://arxiv.org/abs/2009.01325)]
2. **Second thoughts are best: Learning to re-align with human values from text edits** *Liu, Ruibo, Jia, Chenyan, Zhang, Ge, Zhuang, Ziyu, Liu, Tony, Vosoughi, Soroush.* Advances in Neural Information Processing Systems, 2022. [[abs](https://arxiv.org/abs/2301.00355)]
3. **Training language models to follow instructions with human feedback** *Ouyang, Long, Wu, Jeffrey, Jiang, Xu, Almeida, Diogo, Wainwright, Carroll, Mishkin, Pamela, Zhang, Chong, Agarwal, Sandhini, Slama, Katarina, Ray, Alex, others.* Advances in neural information processing systems, 2022. [[abs](https://arxiv.org/abs/2203.02155)]
4. **Leashing the Inner Demons: Self-Detoxification for Language Models** *Xu, Canwen, He, Zexue, He, Zhankui, McAuley, Julian.* Presented at Proceedings of the AAAI Conference on Artificial Intelligence, 2022. [Link](No URL)
5. **Aligning generative language models with human values** *Liu, Ruibo, Zhang, Ge, Feng, Xinyu, Vosoughi, Soroush.* Presented at Findings of the Association for Computational Linguistics: NAACL 2022, 2022. [Link](No URL)
6. **Training a helpful and harmless assistant with reinforcement learning from human feedback** *Bai, Yuntao, Jones, Andy, Ndousse, Kamal, Askell, Amanda, Chen, Anna, DasSarma, Nova, Drain, Dawn, Fort, Stanislav, Ganguli, Deep, Henighan, Tom, others.* arXiv preprint arXiv:2204.05862, 2022. [[abs](https://arxiv.org/abs/2204.05862)]
7. **Constitutional ai: Harmlessness from ai feedback** *Bai, Yuntao, Kadavath, Saurav, Kundu, Sandipan, Askell, Amanda, Kernion, Jackson, Jones, Andy, Chen, Anna, Goldie, Anna, Mirhoseini, Azalia, McKinnon, Cameron, others.* arXiv preprint arXiv:2212.08073, 2022. [[abs](https://arxiv.org/abs/2212.08073)]
8. **Raft: Reward ranked finetuning for generative foundation model alignment** *Dong, Hanze, Xiong, Wei, Goyal, Deepanshu, Pan, Rui, Diao, Shizhe, Zhang, Jipeng, Shum, Kashun, Zhang, Tong.* arXiv preprint arXiv:2304.06767, 2023. [[abs](https://arxiv.org/abs/2304.06767)]
9. **Improving Language Models with Advantage-based Offline Policy Gradients** *Baheti, Ashutosh, Lu, Ximing, Brahman, Faeze, Bras, Ronan Le, Sap, Maarten, Riedl, Mark.* arXiv preprint arXiv:2305.14718, 2023. [[abs](https://arxiv.org/abs/2305.14718)]
10. **Training language models with language feedback at scale** *Scheurer, J\'e.* arXiv preprint arXiv:2303.16755, 2023. [[abs](https://arxiv.org/abs/2303.16755)]
11. **A general theoretical paradigm to understand learning from human preferences** *Azar, Mohammad Gheshlaghi, Rowland, Mark, Piot, Bilal, Guo, Daniel, Calandriello, Daniele, Valko, Michal, Munos, R\'e.* arXiv preprint arXiv:2310.12036, 2023. [[abs](https://arxiv.org/abs/2310.12036)]
12. **Let's Verify Step by Step** *Lightman, Hunter, Kosaraju, Vineet, Burda, Yura, Edwards, Harri, Baker, Bowen, Lee, Teddy, Leike, Jan, Schulman, John, Sutskever, Ilya, Cobbe, Karl.* arXiv preprint arXiv:2305.20050, 2023. [[abs](https://arxiv.org/abs/2305.20050)]
13. **Principled Reinforcement Learning with Human Feedback from Pairwise or $ K $-wise Comparisons** *Zhu, Banghua, Jiao, Jiantao, Jordan, Michael I.* arXiv preprint arXiv:2301.11270, 2023. [[abs](https://arxiv.org/abs/2301.11270)]
14. **Open problems and fundamental limitations of reinforcement learning from human feedback** *Casper, Stephen, Davies, Xander, Shi, Claudia, Gilbert, Thomas Krendl, Scheurer, J\'e.* arXiv preprint arXiv:2307.15217, 2023. [[abs](https://arxiv.org/abs/2307.15217)]
15. **Aligning Large Language Models through Synthetic Feedback** *Kim, Sungdong, Bae, Sanghwan, Shin, Jamin, Kang, Soyoung, Kwak, Donghyun, Yoo, Kang Min, Seo, Minjoon.* arXiv preprint arXiv:2305.13735, 2023. [[abs](https://arxiv.org/abs/2305.13735)]
16. **Rlaif: Scaling reinforcement learning from human feedback with ai feedback** *Lee, Harrison, Phatale, Samrat, Mansoor, Hassan, Lu, Kellie, Mesnard, Thomas, Bishop, Colton, Carbune, Victor, Rastogi, Abhinav.* arXiv preprint arXiv:2309.00267, 2023. [[abs](https://arxiv.org/abs/2309.00267)]
17. **Preference ranking optimization for human alignment** *Song, Feifan, Yu, Bowen, Li, Minghao, Yu, Haiyang, Huang, Fei, Li, Yongbin, Wang, Houfeng.* arXiv preprint arXiv:2306.17492, 2023. [[abs](https://arxiv.org/abs/2306.17492)]
18. **Improving Factuality and Reasoning in Language Models through Multiagent Debate** *Du, Yilun, Li, Shuang, Torralba, Antonio, Tenenbaum, Joshua B, Mordatch, Igor.* arXiv preprint arXiv:2305.14325, 2023. [[abs](https://arxiv.org/abs/2305.14325)]
19. **Large language model alignment: A survey** *Shen, Tianhao, Jin, Renren, Huang, Yufei, Liu, Chuang, Dong, Weilong, Guo, Zishan, Wu, Xinwei, Liu, Yan, Xiong, Deyi.* arXiv preprint arXiv:2309.15025, 2023. [[abs](https://arxiv.org/abs/2309.15025)]
20. **Direct preference optimization: Your language model is secretly a reward model** *Rafailov, Rafael, Sharma, Archit, Mitchell, Eric, Manning, Christopher D, Ermon, Stefano, Finn, Chelsea.* Advances in Neural Information Processing Systems, 2024.
21. **Lima: Less is more for alignment** *Zhou, Chunting, Liu, Pengfei, Xu, Puxin, Iyer, Srinivasan, Sun, Jiao, Mao, Yuning, Ma, Xuezhe, Efrat, Avia, Yu, Ping, Yu, Lili, others.* Advances in Neural Information Processing Systems, 2024. [[abs](https://arxiv.org/abs/2305.11206)]

### 5.3 How to approach AGI Alignments

1. **Ethical and social risks of harm from Language**  arXiv 2021. [[abs](https://arxiv.org/abs/2112.04359)]
2. **Beijing AI Safety International Consensus** *Beijing Academy of Artificial Intelligence.* 2024. [Link](https://news.cgtn.com/news/2024-03-14/VHJhbnNjcmlwdDc3NzI1/index.html)
3. **Counterfactual explanations without opening the black box: Automated decisions and the GDPR** *Wachter, Sandra, Mittelstadt, Brent, Russell, Chris.* Harvard Journal of Law \& Technology, 2017.
4. **Scalable agent alignment via reward modeling: a research direction** *Leike, Jan, Krueger, David, Everitt, Tom, Martic, Miljan, Maini, Vishal, Legg, Shane.* arXiv preprint arXiv:1811.07871, 2018. [[abs](https://arxiv.org/abs/1811.07871)]
5. **Building ethics into artificial intelligence** *Yu, Han, Shen, Zhiqi, Miao, Chunyan, Leung, Cyril, Lesser, Victor R, Yang, Qiang.* Presented at Proceedings of the 27th International Joint Conference on Artificial Intelligence, 2018. [Link](No URL)
6. **Human Compatible: Artificial Intelligence and the Problem of Control** *Russell, Stuart.* Viking, 2019. [Link](https://people.eecs.berkeley.edu/~russell/papers/mi19book-hcai.pdf)
7. **Responsible artificial intelligence: How to develop and use AI in a responsible way** *Dignum, Virginia.* Springer Nature, 2019. [Link](https://link.springer.com/book/10.1007/978-3-030-30371-6)
8. **Machine ethics: The design and governance of ethical AI and autonomous systems** *Winfield, Alan F, Michael, Katina, Pitt, Jeremy, Evers, Vanessa.* Proceedings of the IEEE, 2019. [Link](https://ieeexplore.ieee.org/document/8662743)
9. **Open problems in cooperative AI** *Dafoe, Allan, Hughes, Edward, Bachrach, Yoram, Collins, Tantum, McKee, Kevin R, Leibo, Joel Z, Larson, Kate, Graepel, Thore.* arXiv preprint arXiv:2012.08630, 2020. [[abs](https://arxiv.org/abs/2012.08630)]
10. **Artificial intelligence, values, and alignment** *Gabriel, Iason.* Minds and Machines, 2020. [[abs](https://arxiv.org/abs/2001.09768)]
11. **Artificial Intelligence Safety and Security** *Yampolskiy, Roman V.* CRC Press, 2020. [Link](https://www.routledge.com/Artificial-Intelligence-Safety-and-Security/Yampolskiy/p/book/9780815369820)
12. **Cooperative AI: machines must learn to find common ground** *Dafoe, Allan, Bachrach, Yoram, Hadfield, Gillian, Horvitz, Eric, Larson, Kate, Graepel, Thore.* arXiv 2021. [[abs](https://arxiv.org/abs/No eprint ID)]
13. **Machine morality, moral progress, and the looming environmental disaster** *Kenward, Ben, Sinclair, Thomas.* arXiv 2021. [[abs](https://arxiv.org/abs/No eprint ID)]
14. **X-risk analysis for ai research** *Hendrycks, Dan, Mazeika, Mantas.* arXiv preprint arXiv:2206.05862, 2022. [[abs](https://arxiv.org/abs/2206.05862)]
15. **Task decomposition for scalable oversight (AGISF Distillation)** *Charbel-Rapha√´l Segerie.* arXiv 2023. [[abs](https://arxiv.org/abs/No eprint ID)]
16. **Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision** *Burns, Collin, Izmailov, Pavel, Kirchner, Jan Hendrik, Baker, Bowen, Gao, Leo, Aschenbrenner, Leopold, Chen, Yining, Ecoffet, Adrien, Joglekar, Manas, Leike, Jan, others.* arXiv preprint arXiv:2312.09390, 2023. [[abs](https://arxiv.org/abs/2312.09390)]
17. **Whose opinions do language models reflect?** *Santurkar, Shibani, Durmus, Esin, Ladhak, Faisal, Lee, Cinoo, Liang, Percy, Hashimoto, Tatsunori.* Presented at International Conference on Machine Learning, 2023. [Link](No URL)
18. **Ai alignment: A comprehensive survey** *Ji, Jiaming, Qiu, Tianyi, Chen, Boyuan, Zhang, Borong, Lou, Hantao, Wang, Kaile, Duan, Yawen, He, Zhonghao, Zhou, Jiayi, Zhang, Zhaowei, others.* arXiv preprint arXiv:2310.19852, 2023. [[abs](https://arxiv.org/abs/2310.19852)]
19. **Open problems and fundamental limitations of reinforcement learning from human feedback** *Casper, Stephen, Davies, Xander, Shi, Claudia, Gilbert, Thomas Krendl, Scheurer, J\'e.* arXiv preprint arXiv:2307.15217, 2023. [[abs](https://arxiv.org/abs/2307.15217)]
20. **The unlocking spell on base llms: Rethinking alignment via in-context learning** *Lin, Bill Yuchen, Ravichander, Abhilasha, Lu, Ximing, Dziri, Nouha, Sclar, Melanie, Chandu, Khyathi, Bhagavatula, Chandra, Choi, Yejin.* arXiv preprint arXiv:2312.01552, 2023. [[abs](https://arxiv.org/abs/2312.01552)]
21. **Large language model alignment: A survey** *Shen, Tianhao, Jin, Renren, Huang, Yufei, Liu, Chuang, Dong, Weilong, Guo, Zishan, Wu, Xinwei, Liu, Yan, Xiong, Deyi.* arXiv preprint arXiv:2309.15025, 2023. [[abs](https://arxiv.org/abs/2309.15025)]

## 6. Approach AGI Responsibly
### 6.1 AI Levels: Charting the Evolution of Artificial Intelligence
#### 6.1.1 AGI Levels
#### 6.1.2 Constraints and Challenges of Ultimate AGI
#### 6.1.3 How Do We Get to the Next Level of AGI?
### 6.2 AGI Evaluation
#### 6.2.1 What Do We Expect from AGI Evaluations
#### 6.2.2 Current Evaluation Frameworks and Limitations
### 6.3 Potential Ways to Future AGI

## 7. Case Studies
### 7.1 AI for Science Discovery and Research
### 7.2 Generative Visual Intelligence
1. **Deep Unsupervised Learning using Nonequilibrium Thermodynamics**. *Jascha Sohl-Dickstein* et al. ICML 2015. [[paper](https://arxiv.org/abs/1503.03585)]
2. **Generative Modeling by Estimating Gradients of the Data Distribution**. *Yang Song* et al. NeurIPS 2019. [[paper](https://arxiv.org/abs/1907.05600)]
3. **Denoising Diffusion Probabilistic Models**. *Jonathan Ho* et al. NeurIPS 2020. [[paper](https://arxiv.org/abs/2006.11239)]
4. **Score-Based Generative Modeling through Stochastic Differential Equations**. *Yang Song* et al. ICLR 2021. [[paper](https://arxiv.org/abs/2011.13456)]
5. **GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models**. *Alex Nichol* et al. ICML 2022. [[paper](https://arxiv.org/abs/2112.10741)]
6. **SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations**. *Chenlin Meng* et al. ICLR 2022. [[paper](https://arxiv.org/abs/2108.01073)]
7. **Video Diffusion Models**. *Jonathan Ho* et al. NeurIPS 2022. [[paper](https://arxiv.org/abs/2204.03458)]
8. **Hierarchical Text-Conditional Image Generation with CLIP Latents**. *Aditya Ramesh* et al. arXiv 2022. [[paper](https://arxiv.org/abs/2204.06125)]
9. **Classifier-Free Diffusion Guidance**. *Jonathan Ho* et al. arXiv 2022. [[paper](https://arxiv.org/abs/2207.12598)]
10. **Palette: Image-to-Image Diffusion Models**. *Chitwan Saharia* et al. SIGGRAPH 2022. [[paper](https://arxiv.org/abs/2111.05826)]
11. **High-Resolution Image Synthesis with Latent Diffusion Models**. *Robin Rombach* et al. CVPR 2022. [[paper](https://arxiv.org/abs/2112.10752)]
12. **Adding Conditional Control to Text-to-Image Diffusion Models**. *Lvmin Zhang* et al. ICCV 2023. [[paper](https://arxiv.org/abs/2302.05543)]
13. **Scalable Diffusion Models with Transformers**. *William Peebles* et al. ICCV 2023. [[paper](https://arxiv.org/abs/2212.09748)]
14. **Sequential Modeling Enables Scalable Learning for Large Vision Models**. *Yutong Bai* et al. arXiv 2023. [[paper](https://arxiv.org/abs/2312.00785)]
15. **Video Generation Models as World Simulators**. *Tim Brooks* et al. OpenAI 2024. [[paper](https://openai.com/research/video-generation-models-as-world-simulators)]

### 7.3 World Models
### 7.4 Decentralized LLM

1. **Petals: Collaborative Inference and Fine-tuning of Large Models** *Borzunov, Alexander, Baranchuk, Dmitry, Dettmers, Tim, Ryabinin, Max, Belkada, Younes, Chumachenko, Artem, Samygin, Pavel, Raffel, Colin.* arXiv preprint arXiv:2209.01188, 2022. [[abs](https://arxiv.org/abs/2209.01188)]
2. **Blockchain for Deep Learning: Review and Open Challenges** *The Economic Times.* arXiv 2021. [[abs](https://arxiv.org/abs/No eprint ID)]
3. **FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU** *Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y. Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E. Gonzalez, Percy Liang, Christopher R√©, Ion Stoica, Ce Zhang.* arXiv 2023. [[abs](https://arxiv.org/abs/2303.06865)]
4. **Decentralized Training of Foundation Models in Heterogeneous Environments** *Binhang Yuan, Yongjun He, Jared Quincy Davis, Tianyi Zhang, Tri Dao, Beidi Chen, Percy Liang, Christopher Re, Ce Zhang.* arXiv 2023. [[abs](https://arxiv.org/abs/2206.01288)]

### 7.5 AI for Coding

1. **A framework for the evaluation of code generation models** *Ben Allal et al." arXiv. [[abs](https://arxiv.org/abs/No eprint ID)]
2. **Evaluating Large Language Models Trained on Code** *Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, Wojciech Zaremba.* No journal, 2021.
3. **Program Synthesis with Large Language Models** *Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, Charles Sutton.* arXiv 2021. [[abs](https://arxiv.org/abs/2108.07732)]
4. **Competition-level code generation with AlphaCode** *Li, Yujia, Choi, David, Chung, Junyoung, Kushman, Nate, Schrittwieser, Julian, Leblond, R√©mi, Eccles, Tom, Keeling, James, Gimeno, Felix, Dal Lago, Agustin, Hubert, Thomas, Choy, Peter, de Masson d‚ÄôAutume, Cyprien, Babuschkin, Igor, Chen, Xinyun, Huang, Po-Sen, Welbl, Johannes, Gowal, Sven, Cherepanov, Alexey, Molloy, James, Mankowitz, Daniel J., Sutherland Robson, Esme, Kohli, Pushmeet, de Freitas, Nando, Kavukcuoglu, Koray, Vinyals, Oriol.* Science, 2022.
5. **Efficient Training of Language Models to Fill in the Middle** *Mohammad Bavarian, Heewoo Jun, Nikolas Tezak, John Schulman, Christine McLeavey, Jerry Tworek, Mark Chen.* arXiv 2022. [[abs](https://arxiv.org/abs/2207.14255)]
6. **SantaCoder: don't reach for the stars!** *Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, Joel Lamy Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert, Francesco De Toni, Bernardo Garc√≠a del R√≠o, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de Vries, Leandro von Werra.* Mind, 2023.
7. **StarCoder: may the source be with you!** *Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, Jo√£o Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu√±oz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, Harm de Vries.* arXiv 2023. [[abs](https://arxiv.org/abs/2305.06161)]
8. **Large Language Models for Compiler Optimization** *Chris Cummins, Volker Seeker, Dejan Grubisic, Mostafa Elhoushi, Youwei Liang, Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Kim Hazelwood, Gabriel Synnaeve, Hugh Leather.* arXiv 2023. [[abs](https://arxiv.org/abs/2309.07062)]
9. **Textbooks Are All You Need** *Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C√©sar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, S√©bastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, Yuanzhi Li.* arXiv 2023. [[abs](https://arxiv.org/abs/2306.11644)]
10. **InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback** *John Yang, Akshara Prabhakar, Karthik Narasimhan, Shunyu Yao.* arXiv 2023. [[abs](https://arxiv.org/abs/2306.14898)]
11. **Reinforcement Learning from Automatic Feedback for High-Quality Unit Test Generation** *Benjamin Steenhoek, Michele Tufano, Neel Sundaresan, Alexey Svyatkovskiy.* arXiv 2023. [[abs](https://arxiv.org/abs/2310.02368)]
12. **InCoder: A Generative Model for Code Infilling and Synthesis** *Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, Mike Lewis.* arXiv 2023. [[abs](https://arxiv.org/abs/2204.05999)]
13. **Refining Decompiled C Code with Large Language Models** *Wai Kin Wong, Huaijin Wang, Zongjie Li, Zhibo Liu, Shuai Wang, Qiyi Tang, Sen Nie, Shi Wu.* arXiv 2023. [[abs](https://arxiv.org/abs/2310.06530)]
14. **SWE-bench: Can Language Models Resolve Real-World GitHub Issues?** *Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, Karthik Narasimhan.* arXiv 2023. [[abs](https://arxiv.org/abs/2310.06770)]
15. **EvoPrompting: Language Models for Code-Level Neural Architecture Search** *Angelica Chen, David M. Dohan, David R. So.* arXiv 2023. [[abs](https://arxiv.org/abs/2302.14838)]
16. **Can Large Language Models Identify And Reason About Security Vulnerabilities? Not Yet** *Saad Ullah, Mingji Han, Saurabh Pujar, Hammond Pearce, Ayse Coskun, Gianluca Stringhini.* arXiv 2023. [[abs](https://arxiv.org/abs/2312.12575)]
17. **SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code** *Ziniu Hu, Ahmet Iscen, Aashi Jain, Thomas Kipf, Yisong Yue, David A. Ross, Cordelia Schmid, Alireza Fathi.* arXiv 2024. [[abs](https://arxiv.org/abs/2403.01248)]
18. **DebugBench: Evaluating Debugging Capability of Large Language Models** *Runchu Tian, Yining Ye, Yujia Qin, Xin Cong, Yankai Lin, Yinxu Pan, Yesai Wu, Zhiyuan Liu, Maosong Sun.* arXiv 2024. [[abs](https://arxiv.org/abs/2401.04621)]
19. **AI-assisted Code Authoring at Scale: Fine-tuning, deploying, and mixed methods evaluation** *Vijayaraghavan Murali, Chandra Maddila, Imad Ahmad, Michael Bolin, Daniel Cheng, Negar Ghorbani, Renuka Fernandez, Nachiappan Nagappan, Peter C. Rigby.* arXiv 2024. [[abs](https://arxiv.org/abs/2305.12050)]
20. **Code Llama: Open Foundation Models for Code** *Baptiste Rozi√®re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, J√©r√©my Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D√©fossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, Gabriel Synnaeve.* arXiv 2024. [[abs](https://arxiv.org/abs/2308.12950)]
21. **Can llm already serve as a database interface? a big bench for large-scale database grounded text-to-sqls** *Li, Jinyang, Hui, Binyuan, Qu, Ge, Yang, Jiaxi, Li, Binhua, Li, Bowen, Wang, Bailin, Qin, Bowen, Geng, Ruiying, Huo, Nan, others.* Advances in Neural Information Processing Systems, 2024.


### 7.6 AI for Robotics in Real World Applications
### 7.7 Human-AI Collaboration

1. **CoAuthor: Designing a Human-AI Collaborative Writing Dataset for Exploring Language Model Capabilities**
   *Mina Lee, Percy Liang, Qian Yang*. CHI 2022. [[paper](http://arxiv.org/abs/2201.06796)]
2. **A Design Space for Intelligent and Interactive Writing Assistants**
   *Mina Lee, Katy Ilonka Gero, John Joon Young Chung, Simon Buckingham Shum, Vipul Raheja, Hua Shen, Subhashini Venugopalan, Thiemo Wambsganss, David Zhou, Emad A. Alghamdi, Tal August, Avinash Bhat, Madiha Zahrah Choksi, Senjuti Dutta, Jin L. C. Guo, Md Naimul Hoque, Yewon Kim, Seyed Parsa Neshaei, Agnia Sergeyuk, Antonette Shibani, Disha Shrivastava, Lila Shroff, Jessi Stark, Sarah Sterman, Sitong Wang, Antoine Bosselut, Daniel Buschek, Joseph Chee Chang, Sherol Chen, Max Kreminski, Joonsuk Park, Roy Pea, Eugenia H. Rho, Shannon Zejiang Shen, Pao Siangliulue*. CHI 2024. [[paper](http://arxiv.org/abs/2403.14117)]
3. **CreativeConnect: Supporting Reference Recombination for Graphic Design Ideation with Generative AI**
   *DaEun Choi, Sumin Hong, Jeongeon Park, John Joon Young Chung, Juho Kim*
   CHI 2024. [[paper](http://arxiv.org/abs/2312.11949)]
4. **I Lead, You Help but Only with Enough Details: Understanding User Experience of Co-Creation with Artificial Intelligence**
   *Changhoon Oh, Jungwoo Song, Jinhan Choi, Seonghyeon Kim, Sungwoo Lee, Bongwon Suh*. CHI 2018. [[paper](https://dl.acm.org/doi/10.1145/3173574.3174223)]
5. **CodeAid: Evaluating a Classroom Deployment of an LLM-based Programming Assistant that Balances Student and Educator Needs**
   *Majeed Kazemitabaar, Runlong Ye, Xiaoning Wang, Austin Z. Henley, Paul Denny, Michelle Craig, Tovi Grossman*. CHI 2024. [[paper](http://arxiv.org/abs/2401.11314)]
6. **AI-Augmented Brainwriting: Investigating the use of LLMs in group ideation**
   *Orit Shaer, Angelora Cooper, Osnat Mokryn, Andrew L. Kun, Hagit Ben Shoshan*. CHI 2024. [[paper](http://arxiv.org/abs/2402.14978)]
7. **Is the Most Accurate AI the Best Teammate? Optimizing AI for Teamwork**
   *Gagan Bansal, Besmira Nushi, Ece Kamar, Eric Horvitz, Daniel S. Weld*. AAAI 2021. [[paper](http://arxiv.org/abs/2004.13102)]
8. **Updates in Human-AI Teams: Understanding and Addressing the Performance/Compatibility Tradeoff**
   *Gagan Bansal, Besmira Nushi, Ece Kamar, Daniel S. Weld, Walter S. Lasecki, Eric Horvitz*. AAAI 2019. [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/4087)]
9. **Harnessing Biomedical Literature to Calibrate Clinicians‚Äô Trust in AI Decision Support Systems**
   *Qian Yang, Yuexing Hao, Kexin Quan, Stephen Yang, Yiran Zhao, Volodymyr Kuleshov, Fei Wang*. CHI 2023. [[paper](https://dl.acm.org/doi/10.1145/3544548.3581393)]
10. **AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts**
      *Tongshuang Wu, Michael Terry, Carrie Jun Cai*. CHI 2022. [[paper](https://dl.acm.org/doi/10.1145/3491102.3517582)]
11. **Designing LLM Chains by Adapting Techniques from Crowdsourcing Workflows**
    *Madeleine Grunde-McLaughlin, Michelle S. Lam, Ranjay Krishna, Daniel S. Weld, Jeffrey Heer*. arXiv, 2023. [[paper](http://arxiv.org/abs/2312.11681)]
12. **Why Johnny Can‚Äôt Prompt: How Non-AI Experts Try (and Fail) to Design LLM Prompts**
    *J.D. Zamfirescu-Pereira, Richmond Y. Wong, Bjoern Hartmann, Qian Yang*. CHI 2023. [[paper](https://dl.acm.org/doi/10.1145/3544548.3581388)]
13. **Designing Theory-Driven User-Centric Explainable AI**
    *Danding Wang, Qian Yang, Ashraf Abdul, Brian Y. Lim*. CHI 2019. [[paper](https://dl.acm.org/doi/10.1145/3290605.3300831)]
14. **Do People Engage Cognitively with AI? Impact of AI Assistance on Incidental Learning**
    *Krzysztof Z. Gajos, Lena Mamykina*. IUI 2022. [[paper](https://dl.acm.org/doi/10.1145/3490099.3511138)]

## 8. Conclusion


<!-- ### Agent & Tool:
- [Tool learning with foundation models](https://arxiv.org/pdf/2304.08354.pdf)
- [A Survey on Large Language Model based Autonomous Agents](https://arxiv.org/pdf/2308.11432.pdf)
- [The Rise and Potential of Large Language Model Based Agents: A Survey](https://arxiv.org/pdf/2309.07864.pdf)
- [Cognitive Architectures for Language Agents](https://arxiv.org/pdf/2309.02427.pdf)

### (Multi-Modal) LLM / Foundation Model:
#### LLM
- [A Survey of Large Language Models](https://arxiv.org/pdf/2303.18223.pdf)
- [A Survey on In-context Learning](https://arxiv.org/pdf/2301.00234.pdf)
- [Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond](https://arxiv.org/pdf/2304.13712.pdf)

#### Multi-Modal LLM
- [A Survey on Multimodal Large Language Models](https://arxiv.org/pdf/2306.13549.pdf)

#### General Foundation Model
- [On the Opportunities and Risks of Foundation Models](https://arxiv.org/pdf/2108.07258.pdf)
- [A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT](https://arxiv.org/pdf/2302.09419.pdf)

### Embodied AI:
#### AGI
- [One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era](https://arxiv.org/pdf/2304.06488.pdf)
- [Sparks of Artificial General Intelligence: Early experiments with GPT-4](https://arxiv.org/pdf/2303.12712.pdf)
- [Towards AGI in Computer Vision: Lessons Learned from GPT and Large Language Models](https://arxiv.org/pdf/2306.08641.pdf)
- [A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT](https://arxiv.org/pdf/2303.04226.pdf)

### Github:
- [LLM-Agent-Paper-List](https://github.com/WooooDyy/LLM-Agent-Paper-List) (Agent)
- [Awesome-AI-Agents](https://github.com/e2b-dev/awesome-ai-agents) (Agent)
- [Awesome-Multimodal-Large-Language-Models](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models) (Multi-modal LLM) -->
